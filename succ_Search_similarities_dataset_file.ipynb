{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjNPvbtWdT0B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjEygKHmdW7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class TestStringMethods(unittest.TestCase):\n",
        "\n",
        "    def test_upper(self):\n",
        "        self.assertEqual('foo'.upper(), 'FOO')\n",
        "\n",
        "    def test_isupper(self):\n",
        "        self.assertTrue('FOO'.isupper())\n",
        "        self.assertFalse('Foo'.isupper())\n",
        "\n",
        "    def test_split(self):\n",
        "        s = 'hello world'\n",
        "        self.assertEqual(s.split(), ['hello', 'world'])\n",
        "        # check that s.split fails when the separator is not a string\n",
        "        with self.assertRaises(TypeError):\n",
        "            s.split(2)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "2FH8ney1dW_Y",
        "outputId": "975774f7-ecec-4272-c3c6-9d03fab764b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E\n",
            "======================================================================\n",
            "ERROR: /root/ (unittest.loader._FailedTest./root/)\n",
            "----------------------------------------------------------------------\n",
            "AttributeError: module '__main__' has no attribute '/root/'\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.003s\n",
            "\n",
            "FAILED (errors=1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "True",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m unittest test_module1 test_module2\n",
        "!python -m unittest test_module.TestClass\n",
        "!python -m unittest test_module.TestClass.test_method"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxPquy8lhYLY",
        "outputId": "978e4607-fa00-4c28-8a35-62749da73e7c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EE\n",
            "======================================================================\n",
            "ERROR: test_module1 (unittest.loader._FailedTest.test_module1)\n",
            "----------------------------------------------------------------------\n",
            "ImportError: Failed to import test module: test_module1\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/unittest/loader.py\", line 162, in loadTestsFromName\n",
            "    module = __import__(module_name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'test_module1'\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ERROR: test_module2 (unittest.loader._FailedTest.test_module2)\n",
            "----------------------------------------------------------------------\n",
            "ImportError: Failed to import test module: test_module2\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/unittest/loader.py\", line 162, in loadTestsFromName\n",
            "    module = __import__(module_name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'test_module2'\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 0.001s\n",
            "\n",
            "FAILED (errors=2)\n",
            "E\n",
            "======================================================================\n",
            "ERROR: test_module (unittest.loader._FailedTest.test_module)\n",
            "----------------------------------------------------------------------\n",
            "ImportError: Failed to import test module: test_module\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/unittest/loader.py\", line 162, in loadTestsFromName\n",
            "    module = __import__(module_name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'test_module'\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.000s\n",
            "\n",
            "FAILED (errors=1)\n",
            "E\n",
            "======================================================================\n",
            "ERROR: test_module (unittest.loader._FailedTest.test_module)\n",
            "----------------------------------------------------------------------\n",
            "ImportError: Failed to import test module: test_module\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/unittest/loader.py\", line 162, in loadTestsFromName\n",
            "    module = __import__(module_name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'test_module'\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.000s\n",
            "\n",
            "FAILED (errors=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greet(name, message=\"Hello\", *args, **kwargs):\n",
        "    \"\"\"Prints a greetings message\"\"\"\n",
        "    print(message, name)\n",
        "    for arg in args:\n",
        "        print(arg)\n",
        "    for key, value in kwargs.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "# Example usage:\n",
        "greet(\"John\", \"Hi\", \"Nice to meet you\", age=30, country=\"USA\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUB6g0dyhoJo",
        "outputId": "536029c5-54ea-4537-b607-7bc5b190ce8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi John\n",
            "Nice to meet you\n",
            "age: 30\n",
            "country: USA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def function_name(parameters):\n",
        "    # function body\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "rzAfYeEImByF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_numbers(num1, num2):\n",
        "    \"\"\"Returns the sum of two numbers\"\"\"\n",
        "    total = num1 + num2\n",
        "    return total\n",
        "\n",
        "# Example usage:\n",
        "result = add_numbers(5, 7)\n",
        "print(result)  # Output: 12\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndOH8EMsmCK8",
        "outputId": "d4b6a262-c3cb-4335-9fde-084abea5a114"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Funktion zum Lesen von parquet-Dateien\n",
        "def lesen_parquet_datei(dateiname):\n",
        "    try:\n",
        "        # Mit Dask lesen wir die Datei in einem Datenrahmen\n",
        "        df = dd.read_parquet(dateiname)\n",
        "        return df.compute()\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler beim Lesen der Datei {dateiname}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Funktion zur Suche nach Ã¤hnlichen Daten\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ãhnlichkeit mithilfe der Kosinus-Ãhnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # RÃ¼ckgabe der Top-5 Ã¤hnlichsten Ergebnisse\n",
        "    return aehnlichkeit.argsort()[0][-5:]\n",
        "\n",
        "# Funktion zum Teilen der Daten in Teile\n",
        "def teilen_daten(daten, teile_groesse):\n",
        "    # Die GrÃ¶Ãe jeder Teilung\n",
        "    teile_groesse_mb = teile_groesse * 1024 * 1024\n",
        "    # Die GesamtgrÃ¶Ãe der Daten\n",
        "    gesamt_groesse = daten.memory_usage().sum()\n",
        "    # Die Anzahl der Teile\n",
        "    anzahl_teile = gesamt_groesse // teile_groesse_mb\n",
        "\n",
        "    # Teile der Daten erstellen\n",
        "    teile = []\n",
        "    for i in range(anzahl_teile):\n",
        "        start = i * teile_groesse_mb\n",
        "        end = (i + 1) * teile_groesse_mb\n",
        "        teile.append(daten.iloc[start:end])\n",
        "\n",
        "    # Restliche Daten\n",
        "    rest = daten.iloc[anzahl_teile * teile_groesse_mb:]\n",
        "    if not rest.empty:\n",
        "        teile.append(rest)\n",
        "\n",
        "    return teile\n",
        "\n",
        "# Haupt-Funktion\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach Ã¤hnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        aehnlichkeit = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.extend(aehnlichkeit)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "# Verwendung\n",
        "dateinamen = [\"datei1.parquet\", \"datei2.parquet\", \"datei3.parquet\"]\n",
        "suche_text = \"Beispieltext\"\n",
        "teile_groesse_mb = 50  # GrÃ¶Ãe jeder Teilung in MB\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "print(\"Ãhnliche Daten:\", aehnliche_daten)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "dolHpji_mEME",
        "outputId": "b725129a-7911-4753-f1bd-c1bb9125675e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fehler beim Lesen der Datei datei1.parquet: An error occurred while calling the read_parquet method registered to the pandas backend.\n",
            "Original Message: /content/datei1.parquet\n",
            "Fehler beim Lesen der Datei datei2.parquet: An error occurred while calling the read_parquet method registered to the pandas backend.\n",
            "Original Message: /content/datei2.parquet\n",
            "Fehler beim Lesen der Datei datei3.parquet: An error occurred while calling the read_parquet method registered to the pandas backend.\n",
            "Original Message: /content/datei3.parquet\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No objects to concatenate",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-35d35d1a5683>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mteile_groesse_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m  \u001b[0;31m# GrÃ¶Ãe jeder Teilung in MB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0maehnliche_daten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhaupt_funktion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdateinamen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuche_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteile_groesse_mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ãhnliche Daten:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maehnliche_daten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-35d35d1a5683>\u001b[0m in \u001b[0;36mhaupt_funktion\u001b[0;34m(dateinamen, suche_text, teile_groesse_mb)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Verkettung der Daten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mdaten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatenframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Teilen der Daten in Teile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_keys_and_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;31m# figure out what our result ndim is going to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m_clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask[dataframe]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN7FpqmapMZU",
        "outputId": "dacfe193-1a30-4d29-f8fb-e9d8c8dfe356"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.11/dist-packages (2024.10.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (8.6.1)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (2.2.2)\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask[dataframe])\n",
            "  Downloading dask_expr-1.1.21-py3-none-any.whl.metadata (2.6 kB)\n",
            "INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading dask_expr-1.1.20-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.18-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]) (17.0.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]) (3.21.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[dataframe]) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[dataframe]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[dataframe]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[dataframe]) (2025.1)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask[dataframe]) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[dataframe]) (1.17.0)\n",
            "Downloading dask_expr-1.1.16-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dask-expr\n",
            "Successfully installed dask-expr-1.1.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m43HtonApZms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Funktion zum Lesen von parquet-Dateien\n",
        "def lesen_parquet_datei(dateiname):\n",
        "    try:\n",
        "        # Mit Dask lesen wir die Datei in einem Datenrahmen\n",
        "        df = dd.read_parquet(dateiname)\n",
        "        return df.compute()\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler beim Lesen der Datei {dateiname}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Funktion zur Suche nach Ã¤hnlichen Daten\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ãhnlichkeit mithilfe der Kosinus-Ãhnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # RÃ¼ckgabe der Top-5 Ã¤hnlichsten Ergebnisse\n",
        "    return aehnlichkeit.argsort()[0][-5:]\n",
        "\n",
        "# Funktion zum Teilen der Daten in Teile\n",
        "def teilen_daten(daten, teile_groesse):\n",
        "    # Die GrÃ¶Ãe jeder Teilung\n",
        "    teile_groesse_mb = teile_groesse * 1024 * 1024\n",
        "    # Die GesamtgrÃ¶Ãe der Daten\n",
        "    gesamt_groesse = daten.memory_usage().sum()\n",
        "    # Die Anzahl der Teile\n",
        "    anzahl_teile = gesamt_groesse // teile_groesse_mb\n",
        "\n",
        "    # Teile der Daten erstellen\n",
        "    teile = []\n",
        "    for i in range(anzahl_teile):\n",
        "        start = i * teile_groesse_mb\n",
        "        end = (i + 1) * teile_groesse_mb\n",
        "        teile.append(daten.iloc[start:end])\n",
        "\n",
        "    # Restliche Daten\n",
        "    rest = daten.iloc[anzahl_teile * teile_groesse_mb:]\n",
        "    if not rest.empty:\n",
        "        teile.append(rest)\n",
        "\n",
        "    return teile\n",
        "\n",
        "# Haupt-Funktion\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. ÃberprÃ¼fen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach Ã¤hnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        aehnlichkeit = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.extend(aehnlichkeit)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "# Verwendung\n",
        "dateinamen = [\"datei1.parquet\", \"datei2.parquet\", \"datei3.parquet\"]\n",
        "suche_text = \"Beispieltext\"\n",
        "teile_groesse_mb = 50  # GrÃ¶Ãe jeder Teilung in MB\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "print(\"Ãhnliche Daten:\", aehnliche_daten)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtUd6H27pZ8M",
        "outputId": "61af23da-73e5-44a0-9d90-dc6ea0197cd3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fehler beim Lesen der Datei datei1.parquet: An error occurred while calling the read_parquet method registered to the pandas backend.\n",
            "Original Message: /content/datei1.parquet\n",
            "Fehler beim Lesen der Datei datei2.parquet: An error occurred while calling the read_parquet method registered to the pandas backend.\n",
            "Original Message: /content/datei2.parquet\n",
            "Fehler beim Lesen der Datei datei3.parquet: An error occurred while calling the read_parquet method registered to the pandas backend.\n",
            "Original Message: /content/datei3.parquet\n",
            "Keine Daten konnten gelesen werden. ÃberprÃ¼fen Sie die Dateinamen und Pfade.\n",
            "Ãhnliche Daten: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/datasets/Salesforce/wikitext/resolve/main/wikitext-2-v1/train-00000-of-00001.parquet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkpLvefpp60N",
        "outputId": "33dcffd6-51ef-47d1-9213-ecea41df3e3f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-24 18:51:39--  https://huggingface.co/datasets/Salesforce/wikitext/resolve/main/wikitext-2-v1/train-00000-of-00001.parquet\n",
            "Resolving huggingface.co (huggingface.co)... 18.244.202.118, 18.244.202.73, 18.244.202.68, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.244.202.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/datasets/wikitext/dfc27e4360c639dc1fba1e403bfffd53af4a5c75d5363b5724d49bf12d07cce6?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27train-00000-of-00001.parquet%3B+filename%3D%22train-00000-of-00001.parquet%22%3B&Expires=1737748299&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzc0ODI5OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYXRhc2V0cy93aWtpdGV4dC9kZmMyN2U0MzYwYzYzOWRjMWZiYTFlNDAzYmZmZmQ1M2FmNGE1Yzc1ZDUzNjNiNTcyNGQ0OWJmMTJkMDdjY2U2P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=PI98vH4meV4GfXYDpDaUmf8AYvpeojXv%7EZTLmK6nJgdRxfUP0BQ0EobShoAgWu00n8WV3beGl7CP6FK8KLZ9VhvA1RtpaFXfcOzAjN49aDein%7Emx3hTch0EZDol04Hj1dJlN2xyrL3bVL6JNhAeAQ9BwDVQKyP7fTXdRun1OT35fzHhOKGEewlHabPavVuWqwm9lpkeuwInxyCTu8kXWti-5angUu4zINs1yFsq86wOJ4uuSxiUBoGu6VkFpw7DH-H6xTfd1-o%7EdG6uKOUphaJajpg8dlxuEJkhxCcjmwvL6MGLe6K1%7EqbJqPJZLgZ103%7EcYoA5WiTLMOVFi7j88rA__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-01-24 18:51:39--  https://cdn-lfs.hf.co/datasets/wikitext/dfc27e4360c639dc1fba1e403bfffd53af4a5c75d5363b5724d49bf12d07cce6?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27train-00000-of-00001.parquet%3B+filename%3D%22train-00000-of-00001.parquet%22%3B&Expires=1737748299&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzc0ODI5OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYXRhc2V0cy93aWtpdGV4dC9kZmMyN2U0MzYwYzYzOWRjMWZiYTFlNDAzYmZmZmQ1M2FmNGE1Yzc1ZDUzNjNiNTcyNGQ0OWJmMTJkMDdjY2U2P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=PI98vH4meV4GfXYDpDaUmf8AYvpeojXv%7EZTLmK6nJgdRxfUP0BQ0EobShoAgWu00n8WV3beGl7CP6FK8KLZ9VhvA1RtpaFXfcOzAjN49aDein%7Emx3hTch0EZDol04Hj1dJlN2xyrL3bVL6JNhAeAQ9BwDVQKyP7fTXdRun1OT35fzHhOKGEewlHabPavVuWqwm9lpkeuwInxyCTu8kXWti-5angUu4zINs1yFsq86wOJ4uuSxiUBoGu6VkFpw7DH-H6xTfd1-o%7EdG6uKOUphaJajpg8dlxuEJkhxCcjmwvL6MGLe6K1%7EqbJqPJZLgZ103%7EcYoA5WiTLMOVFi7j88rA__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.163.115.80, 3.163.115.92, 3.163.115.98, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.163.115.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6068114 (5.8M) [binary/octet-stream]\n",
            "Saving to: âtrain-00000-of-00001.parquetâ\n",
            "\n",
            "train-00000-of-0000 100%[===================>]   5.79M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-01-24 18:51:39 (52.4 MB/s) - âtrain-00000-of-00001.parquetâ saved [6068114/6068114]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Funktion zum Lesen von parquet-Dateien\n",
        "def lesen_parquet_datei(dateiname):\n",
        "    try:\n",
        "        # Mit Dask lesen wir die Datei in einem Datenrahmen\n",
        "        df = dd.read_parquet(dateiname)\n",
        "        return df.compute()\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler beim Lesen der Datei {dateiname}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Funktion zur Suche nach Ã¤hnlichen Daten\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ãhnlichkeit mithilfe der Kosinus-Ãhnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # RÃ¼ckgabe der Top-5 Ã¤hnlichsten Ergebnisse\n",
        "    return aehnlichkeit.argsort()[0][-5:]\n",
        "\n",
        "# Funktion zum Teilen der Daten in Teile\n",
        "def teilen_daten(daten, teile_groesse):\n",
        "    # Die GrÃ¶Ãe jeder Teilung\n",
        "    teile_groesse_mb = teile_groesse * 1024 * 1024\n",
        "    # Die GesamtgrÃ¶Ãe der Daten\n",
        "    gesamt_groesse = daten.memory_usage().sum()\n",
        "    # Die Anzahl der Teile\n",
        "    anzahl_teile = gesamt_groesse // teile_groesse_mb\n",
        "\n",
        "    # Teile der Daten erstellen\n",
        "    teile = []\n",
        "    for i in range(anzahl_teile):\n",
        "        start = i * teile_groesse_mb\n",
        "        end = (i + 1) * teile_groesse_mb\n",
        "        teile.append(daten.iloc[start:end])\n",
        "\n",
        "    # Restliche Daten\n",
        "    rest = daten.iloc[anzahl_teile * teile_groesse_mb:]\n",
        "    if not rest.empty:\n",
        "        teile.append(rest)\n",
        "\n",
        "    return teile\n",
        "\n",
        "# Haupt-Funktion\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. ÃberprÃ¼fen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach Ã¤hnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        aehnlichkeit = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.extend(aehnlichkeit)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "# Verwendung\n",
        "dateinamen = [\"train-00000-of-00001.parquet\"]\n",
        "suche_text = \"Beispieltext\"\n",
        "teile_groesse_mb = 2  # GrÃ¶Ãe jeder Teilung in MB\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "print(\"Ãhnliche Daten:\", aehnliche_daten)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "R3bmsndwp9Ck",
        "outputId": "60a6156f-4f8e-4b83-b269-f03bcabc9545"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by TfidfTransformer.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6e00f79c04c8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mteile_groesse_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m  \u001b[0;31m# GrÃ¶Ãe jeder Teilung in MB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0maehnliche_daten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhaupt_funktion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdateinamen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuche_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteile_groesse_mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ãhnliche Daten:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maehnliche_daten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-6e00f79c04c8>\u001b[0m in \u001b[0;36mhaupt_funktion\u001b[0;34m(dateinamen, suche_text, teile_groesse_mb)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0maehnliche_daten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mteil\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mteile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0maehnlichkeit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuche_aehnliche_daten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuche_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0maehnliche_daten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maehnlichkeit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-6e00f79c04c8>\u001b[0m in \u001b[0;36msuche_aehnliche_daten\u001b[0;34m(daten, suche_text)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msuche_vektor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvektorisierung\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msuche_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Vektorisierung der Daten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdaten_vektor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvektorisierung\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdaten\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Berechnung der Ãhnlichkeit mithilfe der Kosinus-Ãhnlichkeit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1698\u001b[0m         \"\"\"\n\u001b[1;32m   1699\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1700\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m   1701\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1131\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by TfidfTransformer."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø´ØºØ§Ù"
      ],
      "metadata": {
        "id": "fj8_8AXOqk6s"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Funktion zum Lesen von parquet-Dateien\n",
        "def lesen_parquet_datei(dateiname):\n",
        "    try:\n",
        "        # Mit Dask lesen wir die Datei in einem Datenrahmen\n",
        "        df = dd.read_parquet(dateiname)\n",
        "        return df.compute()\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler beim Lesen der Datei {dateiname}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Funktion zur Suche nach Ã¤hnlichen Daten\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ãhnlichkeit mithilfe der Kosinus-Ãhnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # RÃ¼ckgabe der Top-5 Ã¤hnlichsten Ergebnisse\n",
        "    return aehnlichkeit.argsort()[0][-5:]\n",
        "\n",
        "# Funktion zum Teilen der Daten in Teile\n",
        "def teilen_daten(daten, teile_groesse):\n",
        "    # Die GrÃ¶Ãe jeder Teilung\n",
        "    teile_groesse_mb = teile_groesse * 1024 * 1024\n",
        "    # Die GesamtgrÃ¶Ãe der Daten\n",
        "    gesamt_groesse = daten.memory_usage().sum()\n",
        "    # Die Anzahl der Teile\n",
        "    anzahl_teile = int(gesamt_groesse // teile_groesse_mb)  # Ensure integer division\n",
        "\n",
        "    # Teile der Daten erstellen\n",
        "    teile = []\n",
        "    for i in range(anzahl_teile):\n",
        "        start = i * teile_groesse_mb\n",
        "        end = min((i + 1) * teile_groesse_mb, len(daten))  # Limit end index\n",
        "        # Use loc instead of iloc to slice by row label rather than row number\n",
        "        teil = daten.loc[start:end]  # Use loc to slice by row label\n",
        "        if not teil.empty:  # Only add non-empty chunks\n",
        "            teile.append(teil)\n",
        "\n",
        "    # Restliche Daten (handle if the last chunk is smaller than teile_groesse_mb)\n",
        "    if len(daten) > anzahl_teile * teile_groesse_mb:\n",
        "        rest = daten.loc[anzahl_teile * teile_groesse_mb:]\n",
        "        if not rest.empty:\n",
        "            teile.append(rest)\n",
        "\n",
        "    return teile\n",
        "\n",
        "# Haupt-Funktion\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. ÃberprÃ¼fen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach Ã¤hnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        aehnlichkeit = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.extend(aehnlichkeit)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "# Verwendung\n",
        "dateinamen = [\"/content/train-00000-of-00001.parquet\"]\n",
        "suche_text = \"Beispieltext\"\n",
        "teile_groesse_mb = 2  # GrÃ¶Ãe jeder Teilung in MB\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "print(\"Ãhnliche Daten:\", aehnliche_daten)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uU444KmqhV0",
        "outputId": "7344db53-e537-4083-d887-08d722e0a388"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ãhnliche Daten: [12240, 12241, 12242, 12236, 36717]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Ø´Ø±Ø­ Ø§ÙÙÙØ¯ Ø¨Ø§ÙÙØºØ© Ø§ÙØ¹Ø±Ø¨ÙØ©:\n",
        "ÙÙÙÙ Ø§ÙÙÙØ¯ Ø¨ØªØ­ÙÙÙ ÙÙÙØ§Øª Ø§ÙØ¨ÙØ§ÙØ§Øª Ø¨ØªÙØ³ÙÙ \"parquet\" ÙÙØ¨Ø­Ø« Ø¹Ù ÙØµÙØµ ÙØ´Ø§Ø¨ÙØ© ÙÙØµ ÙØ¹ÙÙ ÙÙØ­Ø¯Ø¯Ù Ø§ÙÙØ³ØªØ®Ø¯Ù. Ø¥ÙÙÙ Ø´Ø±Ø­ ÙØ¨Ø³Ø· ÙÙÙ Ø¬Ø²Ø¡ ÙÙ Ø§ÙÙÙØ¯:\n",
        "\n",
        "1. Ø§Ø³ØªÙØ±Ø§Ø¯ Ø§ÙÙÙØªØ¨Ø§Øª:\n",
        "\n",
        "pandas: ÙÙØªÙÙØ© Ø§ÙØ¨ÙØ§ÙØ§Øª ÙØªØ­ÙÙÙÙØ§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù Ø¥Ø·Ø§Ø± Ø¨ÙØ§ÙØ§Øª ÙÙØ³ÙÙ \"DataFrame\".\n",
        "dask.dataframe: ÙÙØªØ¹Ø§ÙÙ ÙØ¹ ÙØ¬ÙÙØ¹Ø§Øª Ø§ÙØ¨ÙØ§ÙØ§Øª Ø§ÙÙØ¨ÙØ±Ø© Ø§ÙØªÙ ÙØ¯ ÙØ§ ØªØªØ³Ø¹ ÙÙ Ø§ÙØ°Ø§ÙØ±Ø©.\n",
        "TfidfVectorizer: ÙØªØ­ÙÙÙ Ø§ÙÙØµÙØµ Ø¥ÙÙ ÙØªØ¬ÙØ§Øª Ø±ÙÙÙØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù Ø®ÙØ§Ø±Ø²ÙÙØ© TF-IDFØ ÙØ§ÙØªÙ ØªØ³Ø§Ø¹Ø¯ ÙÙ ØªØ­Ø¯ÙØ¯ Ø£ÙÙÙØ© Ø§ÙÙÙÙØ§Øª ÙÙ Ø§ÙÙØµ.\n",
        "cosine_similarity: ÙØ­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù Ø§ÙÙØµÙØµ Ø¨ÙØ§Ø¡Ù Ø¹ÙÙ ÙØªØ¬ÙØ§ØªÙØ§ Ø§ÙØ±ÙÙÙØ©.\n",
        "os: ÙÙØªÙØ§Ø¹Ù ÙØ¹ ÙØ¸Ø§Ù Ø§ÙØªØ´ØºÙÙØ ÙØ«Ù Ø§ÙØªØ¹Ø§ÙÙ ÙØ¹ ÙØ³Ø§Ø±Ø§Øª Ø§ÙÙÙÙØ§Øª.\n",
        "2. Ø¯Ø§ÙØ© lesen_parquet_datei:\n",
        "\n",
        "ØªÙØ±Ø£ ÙÙÙ \"parquet\" ÙØªØ­ÙÙÙ Ø¥ÙÙ Ø¥Ø·Ø§Ø± Ø¨ÙØ§ÙØ§Øª \"DataFrame\".\n",
        "ØªØ³ØªØ®Ø¯Ù ÙÙØªØ¨Ø© \"Dask\" ÙÙÙØ±Ø§Ø¡Ø© Ø¨ÙÙØ§Ø¡Ø©Ø Ø«Ù ØªÙØ­ÙÙÙ Ø§ÙÙØªÙØ¬Ø© Ø¥ÙÙ \"DataFrame\" Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù .compute().\n",
        "ÙÙ Ø­Ø§Ù Ø­Ø¯ÙØ« Ø®Ø·Ø£Ø ØªØ·Ø¨Ø¹ Ø±Ø³Ø§ÙØ© Ø®Ø·Ø£ ÙØªÙØ¹ÙØ¯ Ø§ÙÙÙÙØ© \"None\".\n",
        "3. Ø¯Ø§ÙØ© suche_aehnliche_daten:\n",
        "\n",
        "Ø¬ÙÙØ± Ø¹ÙÙÙØ© Ø§ÙØ¨Ø­Ø« Ø¹Ù Ø§ÙÙØµÙØµ Ø§ÙÙØ´Ø§Ø¨ÙØ©.\n",
        "ØªØ³ØªÙØ¨Ù Ø¥Ø·Ø§Ø± Ø¨ÙØ§ÙØ§Øª \"DataFrame\" ÙÙØµ Ø§ÙØ¨Ø­Ø« ÙÙØ¯Ø®ÙØ§Øª.\n",
        "ØªÙÙÙØ¦ TfidfVectorizer ÙØªØ­ÙÙÙ Ø§ÙÙØµÙØµ Ø¥ÙÙ ÙØªØ¬ÙØ§Øª Ø±ÙÙÙØ©Ø ÙØ¹ Ø¥Ø²Ø§ÙØ© Ø§ÙÙÙÙØ§Øª Ø§ÙØ´Ø§Ø¦Ø¹Ø© Ø¨Ø§ÙÙØºØ© Ø§ÙØ¥ÙØ¬ÙÙØ²ÙØ© ÙØ«Ù \"the\" Ù \"a\" Ù \"is\" Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù stop_words=\"english\".\n",
        "ØªÙØ­ÙÙÙ ÙØµ Ø§ÙØ¨Ø­Ø« Ø¥ÙÙ ÙØªØ¬Ù Ø±ÙÙÙ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù fit_transform.\n",
        "ØªÙØ­ÙÙÙ Ø¹ÙÙØ¯ \"text\" ÙÙ Ø¥Ø·Ø§Ø± Ø§ÙØ¨ÙØ§ÙØ§Øª Ø¥ÙÙ ÙØªØ¬ÙØ§Øª Ø±ÙÙÙØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù transform.\n",
        "ØªØ­Ø³Ø¨ ØªØ´Ø§Ø¨Ù Ø§ÙÙØµÙØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù cosine_similarity.\n",
        "ØªÙØ¹ÙØ¯ ÙØ¤Ø´Ø±Ø§Øª Ø§ÙÙØµÙØµ Ø§ÙØ®ÙØ³Ø© Ø§ÙØ£ÙØ«Ø± ØªØ´Ø§Ø¨ÙÙØ§ ÙØ¹ ÙØµ Ø§ÙØ¨Ø­Ø«.\n",
        "4. Ø¯Ø§ÙØ© teilen_daten:\n",
        "\n",
        "ØªÙÙØ³ÙÙ Ø¥Ø·Ø§Ø± Ø§ÙØ¨ÙØ§ÙØ§Øª Ø¥ÙÙ Ø£Ø¬Ø²Ø§Ø¡ Ø£ØµØºØ± ÙØªØ³ÙÙÙ Ø§ÙØªØ¹Ø§ÙÙ ÙØ¹ ÙØ¬ÙÙØ¹Ø§Øª Ø§ÙØ¨ÙØ§ÙØ§Øª Ø§ÙÙØ¨ÙØ±Ø©.\n",
        "ØªØ­Ø³Ø¨ Ø­Ø¬Ù ÙÙ Ø¬Ø²Ø¡ Ø¨ÙØ§Ø¡Ù Ø¹ÙÙ teile_groesse_mb ÙØ­Ø¬Ù Ø§ÙØ°Ø§ÙØ±Ø© Ø§ÙÙÙØ³ØªØ®Ø¯Ù ÙÙ Ø¥Ø·Ø§Ø± Ø§ÙØ¨ÙØ§ÙØ§Øª.\n",
        "ØªØ³ØªØ®Ø¯Ù loc ÙÙØªÙØ³ÙÙ Ø¨ÙØ§Ø¡Ù Ø¹ÙÙ ÙÙØ³ÙÙØ§Øª Ø§ÙØµÙÙÙ.\n",
        "ØªÙØ¹Ø§ÙØ¬ Ø­Ø§ÙØ§Øª Ø§ÙØ¬Ø²Ø¡ Ø§ÙØ£Ø®ÙØ± Ø§ÙØ°Ù ÙØ¯ ÙÙÙÙ Ø£ØµØºØ± ÙÙ Ø§ÙØ­Ø¬Ù Ø§ÙÙÙØ­Ø¯Ø¯.\n",
        "5. Ø¯Ø§ÙØ© haupt_funktion:\n",
        "\n",
        "Ø§ÙØ¯Ø§ÙØ© Ø§ÙØ±Ø¦ÙØ³ÙØ© Ø§ÙØªÙ ØªÙØ¯ÙØ± Ø¹ÙÙÙØ© ÙØ±Ø§Ø¡Ø© Ø§ÙØ¨ÙØ§ÙØ§ØªØ ÙØªÙØ³ÙÙÙØ§Ø ÙØ§ÙØ¨Ø­Ø« Ø¹Ù ÙØµÙØµ ÙØ´Ø§Ø¨ÙØ©.\n",
        "ØªØ³ØªÙØ¨Ù Ø£Ø³ÙØ§Ø¡ Ø§ÙÙÙÙØ§ØªØ ÙØµ Ø§ÙØ¨Ø­Ø«Ø ÙØ­Ø¬Ù Ø§ÙØ¬Ø²Ø¡ ÙÙØ¯Ø®ÙØ§Øª.\n",
        "ØªÙØ±Ø£ ÙÙÙØ§Øª \"parquet\" Ø ÙØªØ¯ÙØ¬ÙØ§ ÙÙ Ø¥Ø·Ø§Ø± Ø¨ÙØ§ÙØ§Øª ÙØ§Ø­Ø¯.\n",
        "ØªÙÙØ³ÙÙ Ø¥Ø·Ø§Ø± Ø§ÙØ¨ÙØ§ÙØ§Øª Ø¥ÙÙ Ø£Ø¬Ø²Ø§Ø¡.\n",
        "ØªØ³ØªØ¯Ø¹Ù Ø¯Ø§ÙØ© suche_aehnliche_daten Ø¹ÙÙ ÙÙ Ø¬Ø²Ø¡ ÙÙØ¨Ø­Ø« Ø¹Ù Ø§ÙÙØµÙØµ Ø§ÙÙØ´Ø§Ø¨ÙØ©.\n",
        "ØªØ¬ÙØ¹ Ø§ÙÙØªØ§Ø¦Ø¬ ÙÙ Ø¬ÙÙØ¹ Ø§ÙØ£Ø¬Ø²Ø§Ø¡ ÙØªÙØ¹ÙØ¯ÙØ§.\n",
        "6. Ø§Ø³ØªØ®Ø¯Ø§Ù Ø§ÙÙÙØ¯:\n",
        "\n",
        "ÙÙØ­Ø¯Ø¯ Ø£Ø³ÙØ§Ø¡ Ø§ÙÙÙÙØ§ØªØ ÙØµ Ø§ÙØ¨Ø­Ø«Ø ÙØ­Ø¬Ù Ø§ÙØ¬Ø²Ø¡.\n",
        "ÙØ³ØªØ¯Ø¹Ù Ø¯Ø§ÙØ© haupt_funktion ÙÙØ­ØµÙÙ Ø¹ÙÙ Ø§ÙÙØªØ§Ø¦Ø¬.\n",
        "ÙØ·Ø¨Ø¹ Ø§ÙÙØªØ§Ø¦Ø¬.\n",
        "ÙØ¹ÙÙ Ø§ÙÙØªÙØ¬Ø©:\n",
        "Ø¹ÙØ¯ÙØ§ ØªØ­ØµÙ Ø¹ÙÙ ÙØ§ØªØ¬ ÙØ«Ù \"Ãhnliche Daten: [12240, 12241, 12242, 12236, 36717]\"Ø ÙÙØ°Ø§ ÙØ¹ÙÙ Ø£Ù Ø§ÙÙÙØ¯ ÙØ¬Ø¯ Ø®ÙØ³Ø© ÙØµÙØµ ÙÙ ÙÙÙØ§Øª \"parquet\" ØªØ´Ø¨Ù ÙØµ Ø§ÙØ¨Ø­Ø« \"Beispieltext\".\n",
        "\n",
        "Ø§ÙØ£Ø±ÙØ§Ù ÙÙ Ø§ÙÙØ§Ø¦ÙØ© ÙÙ ÙÙØ¤Ø´Ø±Ø§Øª (Ø£Ø±ÙØ§Ù Ø§ÙØµÙÙÙ) ÙÙØ°Ù Ø§ÙÙØµÙØµ Ø§ÙÙØ´Ø§Ø¨ÙØ© Ø¯Ø§Ø®Ù Ø¥Ø·Ø§Ø± Ø§ÙØ¨ÙØ§ÙØ§Øª.\n",
        "Ø¨ÙØ¹ÙÙ Ø¢Ø®Ø±Ø Ø§ÙØµÙÙÙ Ø°Ø§Øª Ø§ÙØ£Ø±ÙØ§Ù 12240Ø 12241Ø 12242Ø 12236Ø Ù 36717 ØªØ­ØªÙÙ Ø¹ÙÙ ÙØµÙØµ ØªØ´Ø¨Ù \"Beispieltext\" Ø¨ÙØ§Ø¡Ù Ø¹ÙÙ Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù Ø§ÙÙØµÙØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù Ø®ÙØ§Ø±Ø²ÙÙØ© TF-IDF.\n",
        "Ø¨Ø¨Ø³Ø§Ø·Ø©: ÙÙØ¯ Ø¨Ø­Ø«Øª Ø¹Ù ÙØµ ÙØ´Ø§Ø¨Ù ÙÙ \"Beispieltext\" ÙÙ Ø¨ÙØ§ÙØ§ØªÙØ ÙÙØ¬Ø¯ Ø§ÙÙÙØ¯ Ø®ÙØ³Ø© ØµÙÙÙ ÙÙ Ø¨ÙØ§ÙØ§ØªÙ ØªÙØ·Ø§Ø¨Ù Ø¨Ø­Ø«Ù Ø¨Ø´ÙÙ ÙØ¨ÙØ±. Ø§ÙØ£Ø±ÙØ§Ù ÙÙ Ø§ÙÙØ§ØªØ¬ ØªÙØ´ÙØ± Ø¥ÙÙ ÙÙØ§ÙØ¹ ÙØ°Ù Ø§ÙØµÙÙÙ ÙÙ Ø¨ÙØ§ÙØ§ØªÙ.\n",
        "\n"
      ],
      "metadata": {
        "id": "tD0bpfHWrEE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø´ØºØ§Ù"
      ],
      "metadata": {
        "id": "oJDj-Cz_r4Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Funktion zum Lesen von parquet-Dateien\n",
        "def lesen_parquet_datei(dateiname):\n",
        "    try:\n",
        "        # Mit Dask lesen wir die Datei in einem Datenrahmen\n",
        "        df = dd.read_parquet(dateiname)\n",
        "        return df.compute()\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler beim Lesen der Datei {dateiname}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Funktion zur Suche nach Ã¤hnlichen Daten\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ãhnlichkeit mithilfe der Kosinus-Ãhnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # RÃ¼ckgabe der Top-5 Ã¤hnlichsten Ergebnisse\n",
        "    return aehnlichkeit.argsort()[0][-5:]\n",
        "\n",
        "# Funktion zum Teilen der Daten in Teile\n",
        "def teilen_daten(daten, teile_groesse):\n",
        "    # Die GrÃ¶Ãe jeder Teilung\n",
        "    teile_groesse_mb = teile_groesse * 1024 * 1024\n",
        "    # Die GesamtgrÃ¶Ãe der Daten\n",
        "    gesamt_groesse = daten.memory_usage().sum()\n",
        "    # Die Anzahl der Teile\n",
        "    anzahl_teile = int(gesamt_groesse // teile_groesse_mb)  # Ensure integer division\n",
        "\n",
        "    # Teile der Daten erstellen\n",
        "    teile = []\n",
        "    for i in range(anzahl_teile):\n",
        "        start = i * teile_groesse_mb\n",
        "        end = min((i + 1) * teile_groesse_mb, len(daten))  # Limit end index\n",
        "        # Use loc instead of iloc to slice by row label rather than row number\n",
        "        teil = daten.loc[start:end]  # Use loc to slice by row label\n",
        "        if not teil.empty:  # Only add non-empty chunks\n",
        "            teile.append(teil)\n",
        "\n",
        "    # Restliche Daten (handle if the last chunk is smaller than teile_groesse_mb)\n",
        "    if len(daten) > anzahl_teile * teile_groesse_mb:\n",
        "        rest = daten.loc[anzahl_teile * teile_groesse_mb:]\n",
        "        if not rest.empty:\n",
        "            teile.append(rest)\n",
        "\n",
        "    return teile\n",
        "\n",
        "# Haupt-Funktion\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. ÃberprÃ¼fen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach Ã¤hnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        aehnlichkeit = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.extend(aehnlichkeit)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "# Verwendung\n",
        "dateinamen = [\"/content/train-00000-of-00001.parquet\"]\n",
        "suche_text = \" Valkyira Chronicles\"\n",
        "teile_groesse_mb = 2  # GrÃ¶Ãe jeder Teilung in MB\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "print(\"Ãhnliche Daten:\", aehnliche_daten)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IERVW7DQqz8M",
        "outputId": "04691abd-7490-4071-fe8f-1680d39cac23"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ãhnliche Daten: [26, 10068, 13322, 21, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ÙØªØºÙÙØ± Ø§ÙÙØµ Ø§ÙÙØ±Ø§Ø¯ Ø§ÙØ¨Ø­Ø« Ø¹ÙÙØ ÙØ§ Ø¹ÙÙÙ Ø³ÙÙ ØªØ¹Ø¯ÙÙ ÙÙÙØ© ÙØ°Ø§ Ø§ÙÙØªØºÙØ±.\n",
        "\n",
        "Ø³ØªØ¬Ø¯ Ø§ÙØ³Ø·Ø± Ø§ÙØªØ§ÙÙ ÙÙ Ø§ÙÙÙØ¯:\n",
        "\n",
        "\n",
        "suche_text = \"Beispieltext\""
      ],
      "metadata": {
        "id": "sHZd4g5Hrfe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø´ØºØ§Ù"
      ],
      "metadata": {
        "id": "QfTkvEIwsha0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ãhnlichkeit mithilfe der Kosinus-Ãhnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # RÃ¼ckgabe der Top-5 Ã¤hnlichsten Ergebnisse\n",
        "    idx = aehnlichkeit.argsort()[0][-5:]\n",
        "    return daten.iloc[idx]\n",
        "\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. ÃberprÃ¼fen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach Ã¤hnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        gefunden = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.append(gefunden)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "for i, gefunden in enumerate(aehnliche_daten):\n",
        "    print(f\"Ãhnliche Daten {i+1}:\")\n",
        "    print(gefunden)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI3E9-gBrgzl",
        "outputId": "440f5957-83f7-44e6-d2be-2e31583b927c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ãhnliche Daten 1:\n",
            "                                                    text\n",
            "26      The music was composed by Hitoshi Sakimoto , ...\n",
            "10068   Harold Innis 's interest in the relationship ...\n",
            "13322   Some of the original stories were well @-@ re...\n",
            "21      Concept work for Valkyria Chronicles III bega...\n",
            "46      Valkyria Chronicles 3 was adapted into a two ...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0hPAvSsqsjCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HMP2nW5Qsi_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2EsVCTbasi6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gPA-YD9Gsi3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø´ØºØ§Ù1"
      ],
      "metadata": {
        "id": "hqboVbobtXCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Funktion zum Lesen von parquet-Dateien\n",
        "def lesen_parquet_datei(dateiname):\n",
        "    try:\n",
        "        # Mit Dask lesen wir die Datei in einem Datenrahmen\n",
        "        df = dd.read_parquet(dateiname)\n",
        "        return df.compute()\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler beim Lesen der Datei {dateiname}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Funktion zur Suche nach Ã¤hnlichen Daten\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ãhnlichkeit mithilfe der Kosinus-Ãhnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # RÃ¼ckgabe der Top-5 Ã¤hnlichsten Ergebnisse\n",
        "    return aehnlichkeit.argsort()[0][-5:]\n",
        "\n",
        "# Funktion zum Teilen der Daten in Teile\n",
        "def teilen_daten(daten, teile_groesse):\n",
        "    # Die GrÃ¶Ãe jeder Teilung\n",
        "    teile_groesse_mb = teile_groesse * 1024 * 1024\n",
        "    # Die GesamtgrÃ¶Ãe der Daten\n",
        "    gesamt_groesse = daten.memory_usage().sum()\n",
        "    # Die Anzahl der Teile\n",
        "    anzahl_teile = int(gesamt_groesse // teile_groesse_mb)  # Ensure integer division\n",
        "\n",
        "    # Teile der Daten erstellen\n",
        "    teile = []\n",
        "    for i in range(anzahl_teile):\n",
        "        start = i * teile_groesse_mb\n",
        "        end = min((i + 1) * teile_groesse_mb, len(daten))  # Limit end index\n",
        "        # Use loc instead of iloc to slice by row label rather than row number\n",
        "        teil = daten.loc[start:end]  # Use loc to slice by row label\n",
        "        if not teil.empty:  # Only add non-empty chunks\n",
        "            teile.append(teil)\n",
        "\n",
        "    # Restliche Daten (handle if the last chunk is smaller than teile_groesse_mb)\n",
        "    if len(daten) > anzahl_teile * teile_groesse_mb:\n",
        "        rest = daten.loc[anzahl_teile * teile_groesse_mb:]\n",
        "        if not rest.empty:\n",
        "            teile.append(rest)\n",
        "\n",
        "    return teile\n",
        "\n",
        "# Haupt-Funktion\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. ÃberprÃ¼fen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach Ã¤hnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        aehnlichkeit = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.extend(aehnlichkeit)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "# Verwendung\n",
        "dateinamen = [\"/content/train-00000-of-00001.parquet\"]\n",
        "suche_text = \" Valkyira Chronicles\"\n",
        "teile_groesse_mb = 2  # GrÃ¶Ãe jeder Teilung in MB\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "print(\"Ãhnliche Daten:\", aehnliche_daten)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a9cb5c-c323-4263-9bb4-eb9d05d42262",
        "id": "cO5gpl2ZsjfC"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ãhnliche Daten: [26, 10068, 13322, 21, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø´ØºØ§Ù2"
      ],
      "metadata": {
        "id": "pmdH4YYDtYs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ãhnlichkeit mithilfe der Kosinus-Ãhnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # RÃ¼ckgabe der Top-5 Ã¤hnlichsten Ergebnisse\n",
        "    idx = aehnlichkeit.argsort()[0][-5:]\n",
        "    return daten.iloc[idx]\n",
        "\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. ÃberprÃ¼fen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach Ã¤hnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        gefunden = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.append(gefunden)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "for i, gefunden in enumerate(aehnliche_daten):\n",
        "    print(f\"Ãhnliche Daten {i+1}:\")\n",
        "    print(gefunden)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef22522-404a-4ba5-dbb1-6b9b1b8d5ddc",
        "id": "AnrYYWgSsjfD"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ãhnliche Daten 1:\n",
            "                                                    text\n",
            "26      The music was composed by Hitoshi Sakimoto , ...\n",
            "10068   Harold Innis 's interest in the relationship ...\n",
            "13322   Some of the original stories were well @-@ re...\n",
            "21      Concept work for Valkyria Chronicles III bega...\n",
            "46      Valkyria Chronicles 3 was adapted into a two ...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-L-HvoxPtaYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ESG33MW7taRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yC5qbg18taOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k0XY73attaL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø´ØºØ§Ù Ø¨Ø­Ø« Ø¹Ù Ø§ÙÙØªØ´Ø§Ø¨ÙØ§Øª ÙÙ ÙÙÙ Ø¯Ø§ØªØ§Ø³ÙØª\n",
        "https://huggingface.co/datasets/Salesforce/wikitext/viewer"
      ],
      "metadata": {
        "id": "6ibGSVTdtfV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Funktion zum Lesen von parquet-Dateien\n",
        "def lesen_parquet_datei(dateiname):\n",
        "    try:\n",
        "        # Mit Dask lesen wir die Datei in einem Datenrahmen\n",
        "        df = dd.read_parquet(dateiname)\n",
        "        return df.compute()\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler beim Lesen der Datei {dateiname}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Funktion zur Suche nach Ã¤hnlichen Daten\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ãhnlichkeit mithilfe der Kosinus-Ãhnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # RÃ¼ckgabe der Top-5 Ã¤hnlichsten Ergebnisse\n",
        "    return aehnlichkeit.argsort()[0][-5:]\n",
        "\n",
        "# Funktion zum Teilen der Daten in Teile\n",
        "def teilen_daten(daten, teile_groesse):\n",
        "    # Die GrÃ¶Ãe jeder Teilung\n",
        "    teile_groesse_mb = teile_groesse * 1024 * 1024\n",
        "    # Die GesamtgrÃ¶Ãe der Daten\n",
        "    gesamt_groesse = daten.memory_usage().sum()\n",
        "    # Die Anzahl der Teile\n",
        "    anzahl_teile = int(gesamt_groesse // teile_groesse_mb)  # Ensure integer division\n",
        "\n",
        "    # Teile der Daten erstellen\n",
        "    teile = []\n",
        "    for i in range(anzahl_teile):\n",
        "        start = i * teile_groesse_mb\n",
        "        end = min((i + 1) * teile_groesse_mb, len(daten))  # Limit end index\n",
        "        # Use loc instead of iloc to slice by row label rather than row number\n",
        "        teil = daten.loc[start:end]  # Use loc to slice by row label\n",
        "        if not teil.empty:  # Only add non-empty chunks\n",
        "            teile.append(teil)\n",
        "\n",
        "    # Restliche Daten (handle if the last chunk is smaller than teile_groesse_mb)\n",
        "    if len(daten) > anzahl_teile * teile_groesse_mb:\n",
        "        rest = daten.loc[anzahl_teile * teile_groesse_mb:]\n",
        "        if not rest.empty:\n",
        "            teile.append(rest)\n",
        "\n",
        "    return teile\n",
        "\n",
        "# Haupt-Funktion\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. ÃberprÃ¼fen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach Ã¤hnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        aehnlichkeit = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.extend(aehnlichkeit)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "# Verwendung\n",
        "dateinamen = [\"/content/train-00000-of-00001.parquet\"]\n",
        "suche_text = \" Valkyira Chronicles\"\n",
        "teile_groesse_mb = 2  # GrÃ¶Ãe jeder Teilung in MB\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "print(\"Ãhnliche Daten:\", aehnliche_daten)\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ãhnlichkeit mithilfe der Kosinus-Ãhnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # RÃ¼ckgabe der Top-5 Ã¤hnlichsten Ergebnisse\n",
        "    idx = aehnlichkeit.argsort()[0][-5:]\n",
        "    return daten.iloc[idx]\n",
        "\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. ÃberprÃ¼fen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach Ã¤hnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        gefunden = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.append(gefunden)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "for i, gefunden in enumerate(aehnliche_daten):\n",
        "    print(f\"Ãhnliche Daten {i+1}:\")\n",
        "    print(gefunden)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af59f3ee-9c97-41b5-abf6-bd23d5245386",
        "id": "QOuaJUNUta0_"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ãhnliche Daten: [26, 10068, 13322, 21, 46]\n",
            "Ãhnliche Daten 1:\n",
            "                                                    text\n",
            "26      The music was composed by Hitoshi Sakimoto , ...\n",
            "10068   Harold Innis 's interest in the relationship ...\n",
            "13322   Some of the original stories were well @-@ re...\n",
            "21      Concept work for Valkyria Chronicles III bega...\n",
            "46      Valkyria Chronicles 3 was adapted into a two ...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/datasets/Salesforce/wikitext/viewer?row=32"
      ],
      "metadata": {
        "id": "Nx-bla1fuuqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The game takes place during the Second Europan War . Gallian Army Squad 422 , also known as \" The Nameless \" , are a penal military unit composed of criminals , foreign deserters , and military offenders whose real names are erased from the records and thereon officially referred to by numbers . Ordered by the Gallian military to perform the most dangerous missions that the Regular Army and Militia will not do , they are nevertheless up to the task , exemplified by their motto , Altaha Abilia , meaning \" Always Ready . \" The three main characters are No.7 Kurt Irving , an army officer falsely accused of treason who wishes to redeem himself ; Ace No.1 Imca , a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and No.13 Riela Marcellis , a seemingly jinxed young woman who is unknowingly a descendant of the Valkyria . Together with their fellow squad members , these three are tasked to fight against a mysterious Imperial unit known as Calamity Raven , consisting of mostly Darcsen soldiers .\n",
        "As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible deniability in order to send them on missions that would otherwise make Gallia lose face in the war . While at times this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain members of the 422nd great distress . One such member , Gusurg , becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven , attached to the ideal of Darcsen independence proposed by their leader , Dahau . At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their own interests . Hounded by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort . This continues until the Nameless 's commanding officer , Ramsey Crowe , who had been kept under house arrest , is escorted to the capital city of Randgriz in order to present evidence exonerating the weary soldiers and expose the real traitor , the Gallian General that had accused Kurt of Treason .\n",
        "Partly due to these events , and partly due to the major losses in manpower Gallia suffers towards the end of the war with the Empire , the Nameless are offered a formal position as a squad in the Gallian Army rather than serve as an anonymous shadow force . This is short @-@ lived , however , as following Maximilian 's defeat , Dahau and Calamity Raven move to activate an ancient Valkyrian super weapon within the Empire , kept secret by their benefactor . Without the support of Maximilian or the chance to prove themselves in the war with Gallia , it is Dahau 's last trump card in creating a new Darcsen nation . As an armed Gallian force invading the Empire just following the two nations ' cease @-@ fire would certainly wreck their newfound peace , Kurt decides to once again make his squad the Nameless , asking Crowe to list himself and all under his command as killed @-@ in @-@ action . Now owing allegiance to none other than themselves , the 422nd confronts Dahau and destroys the Valkyrian weapon . Each member then goes their separate ways in order to begin their lives anew .\n",
        "= = Development = =\n",
        "Concept work for Valkyria Chronicles III began after development finished on Valkyria Chronicles II in early 2010 , with full development beginning shortly after this . The director of Valkyria Chronicles II , Takeshi Ozawa , returned to that role for Valkyria Chronicles III . Development work took approximately one year . After the release of Valkyria Chronicles II , the staff took a look at both the popular response for the game and what they wanted to do next for the series . Like its predecessor , Valkyria Chronicles III was developed for PlayStation Portable : this was due to the team wanting to refine the mechanics created for Valkyria Chronicles II , and they had not come up with the \" revolutionary \" idea that would warrant a new entry for the PlayStation 3 . Speaking in an interview , it was stated that the development team considered Valkyria Chronicles III to be the series ' first true sequel : while Valkyria Chronicles II had required a large amount of trial and error during development due to the platform move , the third game gave them a chance to improve upon the best parts of Valkyria Chronicles II due to being on the same platform . In addition to Sega staff from the previous games , development work was also handled by Media.Vision. The original scenario was written Kazuki Yamanobe , while the script was written by Hiroyuki Fujii , Koichi Majima , Kishiko Miyagi , Seiki Nagakawa and Takayuki Shouji . Its story was darker and more somber than that of its predecessor .\n",
        "The majority of material created for previous games , such as the BLiTZ system and the design of maps , was carried over . Alongside this , improvements were made to the game 's graphics and some elements were expanded , such as map layouts , mission structure , and the number of playable units per mission . A part of this upgrade involved creating unique polygon models for each character 's body . In order to achieve this , the cooperative elements incorporated into the second game were removed , as they took up a large portion of memory space needed for the improvements . They also adjusted the difficulty settings and ease of play so they could appeal to new players while retaining the essential components of the series ' gameplay . The newer systems were decided upon early in development . The character designs were done by Raita Honjou , who had worked on the previous Valkyria Chronicles games . When creating the Nameless Squad , Honjou was faced with the same problem he had had during the first game : the military uniforms essentially destroyed character individuality , despite him needing to create unique characters the player could identify while maintaining a sense of reality within the Valkyria Chronicles world . The main color of the Nameless was black . As with the previous Valkyria games , Valkyria Chronicles III used the CANVAS graphics engine . The anime opening was produced by Production I.G.\n",
        "= = = Music = = =\n",
        "The music was composed by Hitoshi Sakimoto , who had also worked on the previous Valkyria Chronicles games . When he originally heard about the project , he thought it would be a light tone similar to other Valkyria Chronicles games , but found the themes much darker than expected . An early theme he designed around his original vision of the project was rejected . He redid the main theme about seven times through the music production due to this need to reassess the game . The main theme was initially recorded using orchestra , then Sakimoto removed elements such as the guitar and bass , then adjusted the theme using a synthesizer before redoing segments such as the guitar piece on their own before incorporating them into the theme . The rejected main theme was used as a hopeful tune that played during the game 's ending . The battle themes were designed around the concept of a \" modern battle \" divorced from a fantasy scenario by using modern musical instruments , constructed to create a sense of atonality . While Sakimoto was most used to working with synthesized music , he felt that he needed to incorporate live instruments such as orchestra and guitar . The guitar was played by Mitsuhiro Ohta , who also arranged several of the later tracks . The game 's opening theme song , \" If You Wish for ... \" ( ãããåãé¡ãã®ãªã , Moshimo Kimi ga Negauno Nara ) , was sung by Japanese singer May 'n . Its theme was the reason soldiers fought , in particular their wish to protect what was precious to them rather than a sense of responsibility or duty . Its lyrics were written by Seiko Fujibayashi , who had worked on May 'n on previous singles .\n",
        "= = = Release = = =\n",
        "In September 2010 , a teaser website was revealed by Sega , hinting at a new Valkyria Chronicles game . In its September issue , Famitsu listed that SenjÅ no Valkyria 3 would be arriving on the PlayStation Portable . Its first public appearance was at the 2010 Tokyo Game Show ( TGS ) , where a demo was made available for journalists and attendees . During the publicity , story details were kept scant so as not to spoil too much for potential players , along with some of its content still being in flux at the time of its reveal . To promote the game and detail the story leading into the game 's events , an episodic Flash visual novel written by Fujii began release in January 2011 . The game was released January 27 , 2011 . During an interview , the development team said that the game had the capacity for downloadable content ( DLC ) , but that no plans were finalized . Multiple DLC maps , featuring additional missions and recruitable characters , were released between February and April 2011 . An expanded edition of the game , Valkyria Chronicles III Extra Edition , released on November 23 , 2011 . Packaged and sold at a lower price than the original , Extra Edition game with seven additional episodes : three new , three chosen by staff from the game 's DLC , and one made available as a pre @-@ order bonus . People who also owned the original game could transfer their save data between versions .\n",
        "Unlike its two predecessors , Valkyria Chronicles III was not released in the west . According to Sega , this was due to poor sales of Valkyria Chronicles II and the general unpopularity of the PSP in the west . An unofficial fan translation patch began development in February 2012 : players with a copy of Valkyria Chronicles III"
      ],
      "metadata": {
        "id": "Bdos3a31uqZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "662rVVgYuqXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sWnLPrJduqUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ibl4RId3uqQj"
      }
    }
  ]
}