{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjNPvbtWdT0B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjEygKHmdW7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class TestStringMethods(unittest.TestCase):\n",
        "\n",
        "    def test_upper(self):\n",
        "        self.assertEqual('foo'.upper(), 'FOO')\n",
        "\n",
        "    def test_isupper(self):\n",
        "        self.assertTrue('FOO'.isupper())\n",
        "        self.assertFalse('Foo'.isupper())\n",
        "\n",
        "    def test_split(self):\n",
        "        s = 'hello world'\n",
        "        self.assertEqual(s.split(), ['hello', 'world'])\n",
        "        # check that s.split fails when the separator is not a string\n",
        "        with self.assertRaises(TypeError):\n",
        "            s.split(2)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "2FH8ney1dW_Y",
        "outputId": "975774f7-ecec-4272-c3c6-9d03fab764b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E\n",
            "======================================================================\n",
            "ERROR: /root/ (unittest.loader._FailedTest./root/)\n",
            "----------------------------------------------------------------------\n",
            "AttributeError: module '__main__' has no attribute '/root/'\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.003s\n",
            "\n",
            "FAILED (errors=1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "True",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m unittest test_module1 test_module2\n",
        "!python -m unittest test_module.TestClass\n",
        "!python -m unittest test_module.TestClass.test_method"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxPquy8lhYLY",
        "outputId": "978e4607-fa00-4c28-8a35-62749da73e7c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EE\n",
            "======================================================================\n",
            "ERROR: test_module1 (unittest.loader._FailedTest.test_module1)\n",
            "----------------------------------------------------------------------\n",
            "ImportError: Failed to import test module: test_module1\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/unittest/loader.py\", line 162, in loadTestsFromName\n",
            "    module = __import__(module_name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'test_module1'\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ERROR: test_module2 (unittest.loader._FailedTest.test_module2)\n",
            "----------------------------------------------------------------------\n",
            "ImportError: Failed to import test module: test_module2\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/unittest/loader.py\", line 162, in loadTestsFromName\n",
            "    module = __import__(module_name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'test_module2'\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 0.001s\n",
            "\n",
            "FAILED (errors=2)\n",
            "E\n",
            "======================================================================\n",
            "ERROR: test_module (unittest.loader._FailedTest.test_module)\n",
            "----------------------------------------------------------------------\n",
            "ImportError: Failed to import test module: test_module\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/unittest/loader.py\", line 162, in loadTestsFromName\n",
            "    module = __import__(module_name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'test_module'\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.000s\n",
            "\n",
            "FAILED (errors=1)\n",
            "E\n",
            "======================================================================\n",
            "ERROR: test_module (unittest.loader._FailedTest.test_module)\n",
            "----------------------------------------------------------------------\n",
            "ImportError: Failed to import test module: test_module\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/unittest/loader.py\", line 162, in loadTestsFromName\n",
            "    module = __import__(module_name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'test_module'\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.000s\n",
            "\n",
            "FAILED (errors=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greet(name, message=\"Hello\", *args, **kwargs):\n",
        "    \"\"\"Prints a greetings message\"\"\"\n",
        "    print(message, name)\n",
        "    for arg in args:\n",
        "        print(arg)\n",
        "    for key, value in kwargs.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "# Example usage:\n",
        "greet(\"John\", \"Hi\", \"Nice to meet you\", age=30, country=\"USA\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUB6g0dyhoJo",
        "outputId": "536029c5-54ea-4537-b607-7bc5b190ce8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi John\n",
            "Nice to meet you\n",
            "age: 30\n",
            "country: USA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def function_name(parameters):\n",
        "    # function body\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "rzAfYeEImByF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_numbers(num1, num2):\n",
        "    \"\"\"Returns the sum of two numbers\"\"\"\n",
        "    total = num1 + num2\n",
        "    return total\n",
        "\n",
        "# Example usage:\n",
        "result = add_numbers(5, 7)\n",
        "print(result)  # Output: 12\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndOH8EMsmCK8",
        "outputId": "d4b6a262-c3cb-4335-9fde-084abea5a114"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Funktion zum Lesen von parquet-Dateien\n",
        "def lesen_parquet_datei(dateiname):\n",
        "    try:\n",
        "        # Mit Dask lesen wir die Datei in einem Datenrahmen\n",
        "        df = dd.read_parquet(dateiname)\n",
        "        return df.compute()\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler beim Lesen der Datei {dateiname}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Funktion zur Suche nach ähnlichen Daten\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ähnlichkeit mithilfe der Kosinus-Ähnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # Rückgabe der Top-5 ähnlichsten Ergebnisse\n",
        "    return aehnlichkeit.argsort()[0][-5:]\n",
        "\n",
        "# Funktion zum Teilen der Daten in Teile\n",
        "def teilen_daten(daten, teile_groesse):\n",
        "    # Die Größe jeder Teilung\n",
        "    teile_groesse_mb = teile_groesse * 1024 * 1024\n",
        "    # Die Gesamtgröße der Daten\n",
        "    gesamt_groesse = daten.memory_usage().sum()\n",
        "    # Die Anzahl der Teile\n",
        "    anzahl_teile = gesamt_groesse // teile_groesse_mb\n",
        "\n",
        "    # Teile der Daten erstellen\n",
        "    teile = []\n",
        "    for i in range(anzahl_teile):\n",
        "        start = i * teile_groesse_mb\n",
        "        end = (i + 1) * teile_groesse_mb\n",
        "        teile.append(daten.iloc[start:end])\n",
        "\n",
        "    # Restliche Daten\n",
        "    rest = daten.iloc[anzahl_teile * teile_groesse_mb:]\n",
        "    if not rest.empty:\n",
        "        teile.append(rest)\n",
        "\n",
        "    return teile\n",
        "\n",
        "# Haupt-Funktion\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach ähnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        aehnlichkeit = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.extend(aehnlichkeit)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "# Verwendung\n",
        "dateinamen = [\"datei1.parquet\", \"datei2.parquet\", \"datei3.parquet\"]\n",
        "suche_text = \"Beispieltext\"\n",
        "teile_groesse_mb = 50  # Größe jeder Teilung in MB\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "print(\"Ähnliche Daten:\", aehnliche_daten)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "dolHpji_mEME",
        "outputId": "b725129a-7911-4753-f1bd-c1bb9125675e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fehler beim Lesen der Datei datei1.parquet: An error occurred while calling the read_parquet method registered to the pandas backend.\n",
            "Original Message: /content/datei1.parquet\n",
            "Fehler beim Lesen der Datei datei2.parquet: An error occurred while calling the read_parquet method registered to the pandas backend.\n",
            "Original Message: /content/datei2.parquet\n",
            "Fehler beim Lesen der Datei datei3.parquet: An error occurred while calling the read_parquet method registered to the pandas backend.\n",
            "Original Message: /content/datei3.parquet\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No objects to concatenate",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-35d35d1a5683>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mteile_groesse_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m  \u001b[0;31m# Größe jeder Teilung in MB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0maehnliche_daten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhaupt_funktion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdateinamen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuche_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteile_groesse_mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ähnliche Daten:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maehnliche_daten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-35d35d1a5683>\u001b[0m in \u001b[0;36mhaupt_funktion\u001b[0;34m(dateinamen, suche_text, teile_groesse_mb)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Verkettung der Daten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mdaten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatenframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Teilen der Daten in Teile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_keys_and_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;31m# figure out what our result ndim is going to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m_clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask[dataframe]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN7FpqmapMZU",
        "outputId": "dacfe193-1a30-4d29-f8fb-e9d8c8dfe356"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.11/dist-packages (2024.10.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (8.6.1)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (2.2.2)\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask[dataframe])\n",
            "  Downloading dask_expr-1.1.21-py3-none-any.whl.metadata (2.6 kB)\n",
            "INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading dask_expr-1.1.20-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.18-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]) (17.0.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]) (3.21.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[dataframe]) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[dataframe]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[dataframe]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[dataframe]) (2025.1)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask[dataframe]) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[dataframe]) (1.17.0)\n",
            "Downloading dask_expr-1.1.16-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dask-expr\n",
            "Successfully installed dask-expr-1.1.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m43HtonApZms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Funktion zum Lesen von parquet-Dateien\n",
        "def lesen_parquet_datei(dateiname):\n",
        "    try:\n",
        "        # Mit Dask lesen wir die Datei in einem Datenrahmen\n",
        "        df = dd.read_parquet(dateiname)\n",
        "        return df.compute()\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler beim Lesen der Datei {dateiname}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Funktion zur Suche nach ähnlichen Daten\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ähnlichkeit mithilfe der Kosinus-Ähnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # Rückgabe der Top-5 ähnlichsten Ergebnisse\n",
        "    return aehnlichkeit.argsort()[0][-5:]\n",
        "\n",
        "# Funktion zum Teilen der Daten in Teile\n",
        "def teilen_daten(daten, teile_groesse):\n",
        "    # Die Größe jeder Teilung\n",
        "    teile_groesse_mb = teile_groesse * 1024 * 1024\n",
        "    # Die Gesamtgröße der Daten\n",
        "    gesamt_groesse = daten.memory_usage().sum()\n",
        "    # Die Anzahl der Teile\n",
        "    anzahl_teile = gesamt_groesse // teile_groesse_mb\n",
        "\n",
        "    # Teile der Daten erstellen\n",
        "    teile = []\n",
        "    for i in range(anzahl_teile):\n",
        "        start = i * teile_groesse_mb\n",
        "        end = (i + 1) * teile_groesse_mb\n",
        "        teile.append(daten.iloc[start:end])\n",
        "\n",
        "    # Restliche Daten\n",
        "    rest = daten.iloc[anzahl_teile * teile_groesse_mb:]\n",
        "    if not rest.empty:\n",
        "        teile.append(rest)\n",
        "\n",
        "    return teile\n",
        "\n",
        "# Haupt-Funktion\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. Überprüfen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach ähnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        aehnlichkeit = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.extend(aehnlichkeit)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "# Verwendung\n",
        "dateinamen = [\"datei1.parquet\", \"datei2.parquet\", \"datei3.parquet\"]\n",
        "suche_text = \"Beispieltext\"\n",
        "teile_groesse_mb = 50  # Größe jeder Teilung in MB\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "print(\"Ähnliche Daten:\", aehnliche_daten)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtUd6H27pZ8M",
        "outputId": "61af23da-73e5-44a0-9d90-dc6ea0197cd3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fehler beim Lesen der Datei datei1.parquet: An error occurred while calling the read_parquet method registered to the pandas backend.\n",
            "Original Message: /content/datei1.parquet\n",
            "Fehler beim Lesen der Datei datei2.parquet: An error occurred while calling the read_parquet method registered to the pandas backend.\n",
            "Original Message: /content/datei2.parquet\n",
            "Fehler beim Lesen der Datei datei3.parquet: An error occurred while calling the read_parquet method registered to the pandas backend.\n",
            "Original Message: /content/datei3.parquet\n",
            "Keine Daten konnten gelesen werden. Überprüfen Sie die Dateinamen und Pfade.\n",
            "Ähnliche Daten: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/datasets/Salesforce/wikitext/resolve/main/wikitext-2-v1/train-00000-of-00001.parquet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkpLvefpp60N",
        "outputId": "33dcffd6-51ef-47d1-9213-ecea41df3e3f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-24 18:51:39--  https://huggingface.co/datasets/Salesforce/wikitext/resolve/main/wikitext-2-v1/train-00000-of-00001.parquet\n",
            "Resolving huggingface.co (huggingface.co)... 18.244.202.118, 18.244.202.73, 18.244.202.68, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.244.202.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/datasets/wikitext/dfc27e4360c639dc1fba1e403bfffd53af4a5c75d5363b5724d49bf12d07cce6?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27train-00000-of-00001.parquet%3B+filename%3D%22train-00000-of-00001.parquet%22%3B&Expires=1737748299&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzc0ODI5OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYXRhc2V0cy93aWtpdGV4dC9kZmMyN2U0MzYwYzYzOWRjMWZiYTFlNDAzYmZmZmQ1M2FmNGE1Yzc1ZDUzNjNiNTcyNGQ0OWJmMTJkMDdjY2U2P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=PI98vH4meV4GfXYDpDaUmf8AYvpeojXv%7EZTLmK6nJgdRxfUP0BQ0EobShoAgWu00n8WV3beGl7CP6FK8KLZ9VhvA1RtpaFXfcOzAjN49aDein%7Emx3hTch0EZDol04Hj1dJlN2xyrL3bVL6JNhAeAQ9BwDVQKyP7fTXdRun1OT35fzHhOKGEewlHabPavVuWqwm9lpkeuwInxyCTu8kXWti-5angUu4zINs1yFsq86wOJ4uuSxiUBoGu6VkFpw7DH-H6xTfd1-o%7EdG6uKOUphaJajpg8dlxuEJkhxCcjmwvL6MGLe6K1%7EqbJqPJZLgZ103%7EcYoA5WiTLMOVFi7j88rA__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-01-24 18:51:39--  https://cdn-lfs.hf.co/datasets/wikitext/dfc27e4360c639dc1fba1e403bfffd53af4a5c75d5363b5724d49bf12d07cce6?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27train-00000-of-00001.parquet%3B+filename%3D%22train-00000-of-00001.parquet%22%3B&Expires=1737748299&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzc0ODI5OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYXRhc2V0cy93aWtpdGV4dC9kZmMyN2U0MzYwYzYzOWRjMWZiYTFlNDAzYmZmZmQ1M2FmNGE1Yzc1ZDUzNjNiNTcyNGQ0OWJmMTJkMDdjY2U2P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=PI98vH4meV4GfXYDpDaUmf8AYvpeojXv%7EZTLmK6nJgdRxfUP0BQ0EobShoAgWu00n8WV3beGl7CP6FK8KLZ9VhvA1RtpaFXfcOzAjN49aDein%7Emx3hTch0EZDol04Hj1dJlN2xyrL3bVL6JNhAeAQ9BwDVQKyP7fTXdRun1OT35fzHhOKGEewlHabPavVuWqwm9lpkeuwInxyCTu8kXWti-5angUu4zINs1yFsq86wOJ4uuSxiUBoGu6VkFpw7DH-H6xTfd1-o%7EdG6uKOUphaJajpg8dlxuEJkhxCcjmwvL6MGLe6K1%7EqbJqPJZLgZ103%7EcYoA5WiTLMOVFi7j88rA__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.163.115.80, 3.163.115.92, 3.163.115.98, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.163.115.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6068114 (5.8M) [binary/octet-stream]\n",
            "Saving to: ‘train-00000-of-00001.parquet’\n",
            "\n",
            "train-00000-of-0000 100%[===================>]   5.79M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-01-24 18:51:39 (52.4 MB/s) - ‘train-00000-of-00001.parquet’ saved [6068114/6068114]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Funktion zum Lesen von parquet-Dateien\n",
        "def lesen_parquet_datei(dateiname):\n",
        "    try:\n",
        "        # Mit Dask lesen wir die Datei in einem Datenrahmen\n",
        "        df = dd.read_parquet(dateiname)\n",
        "        return df.compute()\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler beim Lesen der Datei {dateiname}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Funktion zur Suche nach ähnlichen Daten\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ähnlichkeit mithilfe der Kosinus-Ähnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # Rückgabe der Top-5 ähnlichsten Ergebnisse\n",
        "    return aehnlichkeit.argsort()[0][-5:]\n",
        "\n",
        "# Funktion zum Teilen der Daten in Teile\n",
        "def teilen_daten(daten, teile_groesse):\n",
        "    # Die Größe jeder Teilung\n",
        "    teile_groesse_mb = teile_groesse * 1024 * 1024\n",
        "    # Die Gesamtgröße der Daten\n",
        "    gesamt_groesse = daten.memory_usage().sum()\n",
        "    # Die Anzahl der Teile\n",
        "    anzahl_teile = gesamt_groesse // teile_groesse_mb\n",
        "\n",
        "    # Teile der Daten erstellen\n",
        "    teile = []\n",
        "    for i in range(anzahl_teile):\n",
        "        start = i * teile_groesse_mb\n",
        "        end = (i + 1) * teile_groesse_mb\n",
        "        teile.append(daten.iloc[start:end])\n",
        "\n",
        "    # Restliche Daten\n",
        "    rest = daten.iloc[anzahl_teile * teile_groesse_mb:]\n",
        "    if not rest.empty:\n",
        "        teile.append(rest)\n",
        "\n",
        "    return teile\n",
        "\n",
        "# Haupt-Funktion\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. Überprüfen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach ähnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        aehnlichkeit = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.extend(aehnlichkeit)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "# Verwendung\n",
        "dateinamen = [\"train-00000-of-00001.parquet\"]\n",
        "suche_text = \"Beispieltext\"\n",
        "teile_groesse_mb = 2  # Größe jeder Teilung in MB\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "print(\"Ähnliche Daten:\", aehnliche_daten)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "R3bmsndwp9Ck",
        "outputId": "60a6156f-4f8e-4b83-b269-f03bcabc9545"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by TfidfTransformer.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6e00f79c04c8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mteile_groesse_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m  \u001b[0;31m# Größe jeder Teilung in MB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0maehnliche_daten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhaupt_funktion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdateinamen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuche_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteile_groesse_mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ähnliche Daten:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maehnliche_daten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-6e00f79c04c8>\u001b[0m in \u001b[0;36mhaupt_funktion\u001b[0;34m(dateinamen, suche_text, teile_groesse_mb)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0maehnliche_daten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mteil\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mteile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0maehnlichkeit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuche_aehnliche_daten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuche_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0maehnliche_daten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maehnlichkeit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-6e00f79c04c8>\u001b[0m in \u001b[0;36msuche_aehnliche_daten\u001b[0;34m(daten, suche_text)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msuche_vektor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvektorisierung\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msuche_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Vektorisierung der Daten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdaten_vektor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvektorisierung\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdaten\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Berechnung der Ähnlichkeit mithilfe der Kosinus-Ähnlichkeit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1698\u001b[0m         \"\"\"\n\u001b[1;32m   1699\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1700\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m   1701\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1131\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by TfidfTransformer."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "fj8_8AXOqk6s"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Funktion zum Lesen von parquet-Dateien\n",
        "def lesen_parquet_datei(dateiname):\n",
        "    try:\n",
        "        # Mit Dask lesen wir die Datei in einem Datenrahmen\n",
        "        df = dd.read_parquet(dateiname)\n",
        "        return df.compute()\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler beim Lesen der Datei {dateiname}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Funktion zur Suche nach ähnlichen Daten\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ähnlichkeit mithilfe der Kosinus-Ähnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # Rückgabe der Top-5 ähnlichsten Ergebnisse\n",
        "    return aehnlichkeit.argsort()[0][-5:]\n",
        "\n",
        "# Funktion zum Teilen der Daten in Teile\n",
        "def teilen_daten(daten, teile_groesse):\n",
        "    # Die Größe jeder Teilung\n",
        "    teile_groesse_mb = teile_groesse * 1024 * 1024\n",
        "    # Die Gesamtgröße der Daten\n",
        "    gesamt_groesse = daten.memory_usage().sum()\n",
        "    # Die Anzahl der Teile\n",
        "    anzahl_teile = int(gesamt_groesse // teile_groesse_mb)  # Ensure integer division\n",
        "\n",
        "    # Teile der Daten erstellen\n",
        "    teile = []\n",
        "    for i in range(anzahl_teile):\n",
        "        start = i * teile_groesse_mb\n",
        "        end = min((i + 1) * teile_groesse_mb, len(daten))  # Limit end index\n",
        "        # Use loc instead of iloc to slice by row label rather than row number\n",
        "        teil = daten.loc[start:end]  # Use loc to slice by row label\n",
        "        if not teil.empty:  # Only add non-empty chunks\n",
        "            teile.append(teil)\n",
        "\n",
        "    # Restliche Daten (handle if the last chunk is smaller than teile_groesse_mb)\n",
        "    if len(daten) > anzahl_teile * teile_groesse_mb:\n",
        "        rest = daten.loc[anzahl_teile * teile_groesse_mb:]\n",
        "        if not rest.empty:\n",
        "            teile.append(rest)\n",
        "\n",
        "    return teile\n",
        "\n",
        "# Haupt-Funktion\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. Überprüfen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach ähnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        aehnlichkeit = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.extend(aehnlichkeit)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "# Verwendung\n",
        "dateinamen = [\"/content/train-00000-of-00001.parquet\"]\n",
        "suche_text = \"Beispieltext\"\n",
        "teile_groesse_mb = 2  # Größe jeder Teilung in MB\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "print(\"Ähnliche Daten:\", aehnliche_daten)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uU444KmqhV0",
        "outputId": "7344db53-e537-4083-d887-08d722e0a388"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ähnliche Daten: [12240, 12241, 12242, 12236, 36717]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "شرح الكود باللغة العربية:\n",
        "يقوم الكود بتحليل ملفات البيانات بتنسيق \"parquet\" للبحث عن نصوص مشابهة لنص معين يُحدده المستخدم. إليك شرح مبسط لكل جزء من الكود:\n",
        "\n",
        "1. استيراد المكتبات:\n",
        "\n",
        "pandas: لمتلفة البيانات وتحليلها باستخدام إطار بيانات يُسمى \"DataFrame\".\n",
        "dask.dataframe: للتعامل مع مجموعات البيانات الكبيرة التي قد لا تتسع في الذاكرة.\n",
        "TfidfVectorizer: لتحويل النصوص إلى متجهات رقمية باستخدام خوارزمية TF-IDF، والتي تساعد في تحديد أهمية الكلمات في النص.\n",
        "cosine_similarity: لحساب تشابه النصوص بناءً على متجهاتها الرقمية.\n",
        "os: للتفاعل مع نظام التشغيل، مثل التعامل مع مسارات الملفات.\n",
        "2. دالة lesen_parquet_datei:\n",
        "\n",
        "تقرأ ملف \"parquet\" وتحوله إلى إطار بيانات \"DataFrame\".\n",
        "تستخدم مكتبة \"Dask\" للقراءة بكفاءة، ثم تُحوّل النتيجة إلى \"DataFrame\" باستخدام .compute().\n",
        "في حال حدوث خطأ، تطبع رسالة خطأ وتُعيد القيمة \"None\".\n",
        "3. دالة suche_aehnliche_daten:\n",
        "\n",
        "جوهر عملية البحث عن النصوص المشابهة.\n",
        "تستقبل إطار بيانات \"DataFrame\" ونص البحث كمدخلات.\n",
        "تُهيئ TfidfVectorizer لتحويل النصوص إلى متجهات رقمية، مع إزالة الكلمات الشائعة باللغة الإنجليزية مثل \"the\" و \"a\" و \"is\" باستخدام stop_words=\"english\".\n",
        "تُحوّل نص البحث إلى متجه رقمي باستخدام fit_transform.\n",
        "تُحوّل عمود \"text\" في إطار البيانات إلى متجهات رقمية باستخدام transform.\n",
        "تحسب تشابه النصوص باستخدام cosine_similarity.\n",
        "تُعيد مؤشرات النصوص الخمسة الأكثر تشابهًا مع نص البحث.\n",
        "4. دالة teilen_daten:\n",
        "\n",
        "تُقسّم إطار البيانات إلى أجزاء أصغر لتسهيل التعامل مع مجموعات البيانات الكبيرة.\n",
        "تحسب حجم كل جزء بناءً على teile_groesse_mb وحجم الذاكرة المُستخدم من إطار البيانات.\n",
        "تستخدم loc للتقسيم بناءً على مُسميات الصفوف.\n",
        "تُعالج حالات الجزء الأخير الذي قد يكون أصغر من الحجم المُحدد.\n",
        "5. دالة haupt_funktion:\n",
        "\n",
        "الدالة الرئيسية التي تُدير عملية قراءة البيانات، وتقسيمها، والبحث عن نصوص مشابهة.\n",
        "تستقبل أسماء الملفات، نص البحث، وحجم الجزء كمدخلات.\n",
        "تقرأ ملفات \"parquet\" ، وتدمجها في إطار بيانات واحد.\n",
        "تُقسّم إطار البيانات إلى أجزاء.\n",
        "تستدعي دالة suche_aehnliche_daten على كل جزء للبحث عن النصوص المشابهة.\n",
        "تجمع النتائج من جميع الأجزاء وتُعيدها.\n",
        "6. استخدام الكود:\n",
        "\n",
        "يُحدد أسماء الملفات، نص البحث، وحجم الجزء.\n",
        "يستدعي دالة haupt_funktion للحصول على النتائج.\n",
        "يطبع النتائج.\n",
        "معنى النتيجة:\n",
        "عندما تحصل على ناتج مثل \"Ähnliche Daten: [12240, 12241, 12242, 12236, 36717]\"، فهذا يعني أن الكود وجد خمسة نصوص في ملفات \"parquet\" تشبه نص البحث \"Beispieltext\".\n",
        "\n",
        "الأرقام في القائمة هي مُؤشرات (أرقام الصفوف) لهذه النصوص المشابهة داخل إطار البيانات.\n",
        "بمعنى آخر، الصفوف ذات الأرقام 12240، 12241، 12242، 12236، و 36717 تحتوي على نصوص تشبه \"Beispieltext\" بناءً على حساب تشابه النصوص باستخدام خوارزمية TF-IDF.\n",
        "ببساطة: لقد بحثت عن نص مشابه لـ \"Beispieltext\" في بياناتك، ووجد الكود خمسة صفوف في بياناتك تُطابق بحثك بشكل كبير. الأرقام في الناتج تُشير إلى مواقع هذه الصفوف في بياناتك.\n",
        "\n"
      ],
      "metadata": {
        "id": "tD0bpfHWrEE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "oJDj-Cz_r4Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Funktion zum Lesen von parquet-Dateien\n",
        "def lesen_parquet_datei(dateiname):\n",
        "    try:\n",
        "        # Mit Dask lesen wir die Datei in einem Datenrahmen\n",
        "        df = dd.read_parquet(dateiname)\n",
        "        return df.compute()\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler beim Lesen der Datei {dateiname}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Funktion zur Suche nach ähnlichen Daten\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ähnlichkeit mithilfe der Kosinus-Ähnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # Rückgabe der Top-5 ähnlichsten Ergebnisse\n",
        "    return aehnlichkeit.argsort()[0][-5:]\n",
        "\n",
        "# Funktion zum Teilen der Daten in Teile\n",
        "def teilen_daten(daten, teile_groesse):\n",
        "    # Die Größe jeder Teilung\n",
        "    teile_groesse_mb = teile_groesse * 1024 * 1024\n",
        "    # Die Gesamtgröße der Daten\n",
        "    gesamt_groesse = daten.memory_usage().sum()\n",
        "    # Die Anzahl der Teile\n",
        "    anzahl_teile = int(gesamt_groesse // teile_groesse_mb)  # Ensure integer division\n",
        "\n",
        "    # Teile der Daten erstellen\n",
        "    teile = []\n",
        "    for i in range(anzahl_teile):\n",
        "        start = i * teile_groesse_mb\n",
        "        end = min((i + 1) * teile_groesse_mb, len(daten))  # Limit end index\n",
        "        # Use loc instead of iloc to slice by row label rather than row number\n",
        "        teil = daten.loc[start:end]  # Use loc to slice by row label\n",
        "        if not teil.empty:  # Only add non-empty chunks\n",
        "            teile.append(teil)\n",
        "\n",
        "    # Restliche Daten (handle if the last chunk is smaller than teile_groesse_mb)\n",
        "    if len(daten) > anzahl_teile * teile_groesse_mb:\n",
        "        rest = daten.loc[anzahl_teile * teile_groesse_mb:]\n",
        "        if not rest.empty:\n",
        "            teile.append(rest)\n",
        "\n",
        "    return teile\n",
        "\n",
        "# Haupt-Funktion\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. Überprüfen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach ähnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        aehnlichkeit = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.extend(aehnlichkeit)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "# Verwendung\n",
        "dateinamen = [\"/content/train-00000-of-00001.parquet\"]\n",
        "suche_text = \" Valkyira Chronicles\"\n",
        "teile_groesse_mb = 2  # Größe jeder Teilung in MB\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "print(\"Ähnliche Daten:\", aehnliche_daten)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IERVW7DQqz8M",
        "outputId": "04691abd-7490-4071-fe8f-1680d39cac23"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ähnliche Daten: [26, 10068, 13322, 21, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "لتغيير النص المراد البحث عنه، ما عليك سوى تعديل قيمة هذا المتغير.\n",
        "\n",
        "ستجد السطر التالي في الكود:\n",
        "\n",
        "\n",
        "suche_text = \"Beispieltext\""
      ],
      "metadata": {
        "id": "sHZd4g5Hrfe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "QfTkvEIwsha0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ähnlichkeit mithilfe der Kosinus-Ähnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # Rückgabe der Top-5 ähnlichsten Ergebnisse\n",
        "    idx = aehnlichkeit.argsort()[0][-5:]\n",
        "    return daten.iloc[idx]\n",
        "\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. Überprüfen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach ähnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        gefunden = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.append(gefunden)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "for i, gefunden in enumerate(aehnliche_daten):\n",
        "    print(f\"Ähnliche Daten {i+1}:\")\n",
        "    print(gefunden)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI3E9-gBrgzl",
        "outputId": "440f5957-83f7-44e6-d2be-2e31583b927c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ähnliche Daten 1:\n",
            "                                                    text\n",
            "26      The music was composed by Hitoshi Sakimoto , ...\n",
            "10068   Harold Innis 's interest in the relationship ...\n",
            "13322   Some of the original stories were well @-@ re...\n",
            "21      Concept work for Valkyria Chronicles III bega...\n",
            "46      Valkyria Chronicles 3 was adapted into a two ...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0hPAvSsqsjCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HMP2nW5Qsi_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2EsVCTbasi6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gPA-YD9Gsi3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال1"
      ],
      "metadata": {
        "id": "hqboVbobtXCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Funktion zum Lesen von parquet-Dateien\n",
        "def lesen_parquet_datei(dateiname):\n",
        "    try:\n",
        "        # Mit Dask lesen wir die Datei in einem Datenrahmen\n",
        "        df = dd.read_parquet(dateiname)\n",
        "        return df.compute()\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler beim Lesen der Datei {dateiname}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Funktion zur Suche nach ähnlichen Daten\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ähnlichkeit mithilfe der Kosinus-Ähnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # Rückgabe der Top-5 ähnlichsten Ergebnisse\n",
        "    return aehnlichkeit.argsort()[0][-5:]\n",
        "\n",
        "# Funktion zum Teilen der Daten in Teile\n",
        "def teilen_daten(daten, teile_groesse):\n",
        "    # Die Größe jeder Teilung\n",
        "    teile_groesse_mb = teile_groesse * 1024 * 1024\n",
        "    # Die Gesamtgröße der Daten\n",
        "    gesamt_groesse = daten.memory_usage().sum()\n",
        "    # Die Anzahl der Teile\n",
        "    anzahl_teile = int(gesamt_groesse // teile_groesse_mb)  # Ensure integer division\n",
        "\n",
        "    # Teile der Daten erstellen\n",
        "    teile = []\n",
        "    for i in range(anzahl_teile):\n",
        "        start = i * teile_groesse_mb\n",
        "        end = min((i + 1) * teile_groesse_mb, len(daten))  # Limit end index\n",
        "        # Use loc instead of iloc to slice by row label rather than row number\n",
        "        teil = daten.loc[start:end]  # Use loc to slice by row label\n",
        "        if not teil.empty:  # Only add non-empty chunks\n",
        "            teile.append(teil)\n",
        "\n",
        "    # Restliche Daten (handle if the last chunk is smaller than teile_groesse_mb)\n",
        "    if len(daten) > anzahl_teile * teile_groesse_mb:\n",
        "        rest = daten.loc[anzahl_teile * teile_groesse_mb:]\n",
        "        if not rest.empty:\n",
        "            teile.append(rest)\n",
        "\n",
        "    return teile\n",
        "\n",
        "# Haupt-Funktion\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. Überprüfen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach ähnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        aehnlichkeit = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.extend(aehnlichkeit)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "# Verwendung\n",
        "dateinamen = [\"/content/train-00000-of-00001.parquet\"]\n",
        "suche_text = \" Valkyira Chronicles\"\n",
        "teile_groesse_mb = 2  # Größe jeder Teilung in MB\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "print(\"Ähnliche Daten:\", aehnliche_daten)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a9cb5c-c323-4263-9bb4-eb9d05d42262",
        "id": "cO5gpl2ZsjfC"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ähnliche Daten: [26, 10068, 13322, 21, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال2"
      ],
      "metadata": {
        "id": "pmdH4YYDtYs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ähnlichkeit mithilfe der Kosinus-Ähnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # Rückgabe der Top-5 ähnlichsten Ergebnisse\n",
        "    idx = aehnlichkeit.argsort()[0][-5:]\n",
        "    return daten.iloc[idx]\n",
        "\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. Überprüfen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach ähnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        gefunden = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.append(gefunden)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "for i, gefunden in enumerate(aehnliche_daten):\n",
        "    print(f\"Ähnliche Daten {i+1}:\")\n",
        "    print(gefunden)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef22522-404a-4ba5-dbb1-6b9b1b8d5ddc",
        "id": "AnrYYWgSsjfD"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ähnliche Daten 1:\n",
            "                                                    text\n",
            "26      The music was composed by Hitoshi Sakimoto , ...\n",
            "10068   Harold Innis 's interest in the relationship ...\n",
            "13322   Some of the original stories were well @-@ re...\n",
            "21      Concept work for Valkyria Chronicles III bega...\n",
            "46      Valkyria Chronicles 3 was adapted into a two ...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-L-HvoxPtaYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ESG33MW7taRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yC5qbg18taOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k0XY73attaL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال بحث عن المتشابهات فى ملف داتاسيت\n",
        "https://huggingface.co/datasets/Salesforce/wikitext/viewer"
      ],
      "metadata": {
        "id": "6ibGSVTdtfV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "# Funktion zum Lesen von parquet-Dateien\n",
        "def lesen_parquet_datei(dateiname):\n",
        "    try:\n",
        "        # Mit Dask lesen wir die Datei in einem Datenrahmen\n",
        "        df = dd.read_parquet(dateiname)\n",
        "        return df.compute()\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler beim Lesen der Datei {dateiname}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Funktion zur Suche nach ähnlichen Daten\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ähnlichkeit mithilfe der Kosinus-Ähnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # Rückgabe der Top-5 ähnlichsten Ergebnisse\n",
        "    return aehnlichkeit.argsort()[0][-5:]\n",
        "\n",
        "# Funktion zum Teilen der Daten in Teile\n",
        "def teilen_daten(daten, teile_groesse):\n",
        "    # Die Größe jeder Teilung\n",
        "    teile_groesse_mb = teile_groesse * 1024 * 1024\n",
        "    # Die Gesamtgröße der Daten\n",
        "    gesamt_groesse = daten.memory_usage().sum()\n",
        "    # Die Anzahl der Teile\n",
        "    anzahl_teile = int(gesamt_groesse // teile_groesse_mb)  # Ensure integer division\n",
        "\n",
        "    # Teile der Daten erstellen\n",
        "    teile = []\n",
        "    for i in range(anzahl_teile):\n",
        "        start = i * teile_groesse_mb\n",
        "        end = min((i + 1) * teile_groesse_mb, len(daten))  # Limit end index\n",
        "        # Use loc instead of iloc to slice by row label rather than row number\n",
        "        teil = daten.loc[start:end]  # Use loc to slice by row label\n",
        "        if not teil.empty:  # Only add non-empty chunks\n",
        "            teile.append(teil)\n",
        "\n",
        "    # Restliche Daten (handle if the last chunk is smaller than teile_groesse_mb)\n",
        "    if len(daten) > anzahl_teile * teile_groesse_mb:\n",
        "        rest = daten.loc[anzahl_teile * teile_groesse_mb:]\n",
        "        if not rest.empty:\n",
        "            teile.append(rest)\n",
        "\n",
        "    return teile\n",
        "\n",
        "# Haupt-Funktion\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. Überprüfen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach ähnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        aehnlichkeit = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.extend(aehnlichkeit)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "# Verwendung\n",
        "dateinamen = [\"/content/train-00000-of-00001.parquet\"]\n",
        "suche_text = \" Valkyira Chronicles\"\n",
        "teile_groesse_mb = 2  # Größe jeder Teilung in MB\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "print(\"Ähnliche Daten:\", aehnliche_daten)\n",
        "def suche_aehnliche_daten(daten, suche_text):\n",
        "    # TF-IDF-Vektorisierung\n",
        "    vektorisierung = TfidfVectorizer(stop_words=\"english\")\n",
        "    # Vektorisierung der Suchtextes\n",
        "    suche_vektor = vektorisierung.fit_transform([suche_text])\n",
        "    # Vektorisierung der Daten\n",
        "    daten_vektor = vektorisierung.transform(daten[\"text\"])\n",
        "\n",
        "    # Berechnung der Ähnlichkeit mithilfe der Kosinus-Ähnlichkeit\n",
        "    aehnlichkeit = cosine_similarity(suche_vektor, daten_vektor)\n",
        "\n",
        "    # Rückgabe der Top-5 ähnlichsten Ergebnisse\n",
        "    idx = aehnlichkeit.argsort()[0][-5:]\n",
        "    return daten.iloc[idx]\n",
        "\n",
        "def haupt_funktion(dateinamen, suche_text, teile_groesse_mb=100):\n",
        "    # Lesen der Dateien\n",
        "    datenframes = []\n",
        "    for dateiname in dateinamen:\n",
        "        daten = lesen_parquet_datei(dateiname)\n",
        "        if daten is not None:\n",
        "            datenframes.append(daten)\n",
        "\n",
        "    # Check if any dataframes were read successfully\n",
        "    if not datenframes:  # If datenframes is empty\n",
        "        print(\"Keine Daten konnten gelesen werden. Überprüfen Sie die Dateinamen und Pfade.\")\n",
        "        return []  # Return an empty list to avoid the ValueError\n",
        "\n",
        "    # Verkettung der Daten\n",
        "    daten = pd.concat(datenframes)\n",
        "\n",
        "    # Teilen der Daten in Teile\n",
        "    teile = teilen_daten(daten, teile_groesse_mb)\n",
        "\n",
        "    # Suche nach ähnlichen Daten\n",
        "    aehnliche_daten = []\n",
        "    for teil in teile:\n",
        "        gefunden = suche_aehnliche_daten(teil, suche_text)\n",
        "        aehnliche_daten.append(gefunden)\n",
        "\n",
        "    return aehnliche_daten\n",
        "\n",
        "aehnliche_daten = haupt_funktion(dateinamen, suche_text, teile_groesse_mb)\n",
        "for i, gefunden in enumerate(aehnliche_daten):\n",
        "    print(f\"Ähnliche Daten {i+1}:\")\n",
        "    print(gefunden)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af59f3ee-9c97-41b5-abf6-bd23d5245386",
        "id": "QOuaJUNUta0_"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ähnliche Daten: [26, 10068, 13322, 21, 46]\n",
            "Ähnliche Daten 1:\n",
            "                                                    text\n",
            "26      The music was composed by Hitoshi Sakimoto , ...\n",
            "10068   Harold Innis 's interest in the relationship ...\n",
            "13322   Some of the original stories were well @-@ re...\n",
            "21      Concept work for Valkyria Chronicles III bega...\n",
            "46      Valkyria Chronicles 3 was adapted into a two ...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/datasets/Salesforce/wikitext/viewer?row=32"
      ],
      "metadata": {
        "id": "Nx-bla1fuuqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The game takes place during the Second Europan War . Gallian Army Squad 422 , also known as \" The Nameless \" , are a penal military unit composed of criminals , foreign deserters , and military offenders whose real names are erased from the records and thereon officially referred to by numbers . Ordered by the Gallian military to perform the most dangerous missions that the Regular Army and Militia will not do , they are nevertheless up to the task , exemplified by their motto , Altaha Abilia , meaning \" Always Ready . \" The three main characters are No.7 Kurt Irving , an army officer falsely accused of treason who wishes to redeem himself ; Ace No.1 Imca , a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and No.13 Riela Marcellis , a seemingly jinxed young woman who is unknowingly a descendant of the Valkyria . Together with their fellow squad members , these three are tasked to fight against a mysterious Imperial unit known as Calamity Raven , consisting of mostly Darcsen soldiers .\n",
        "As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible deniability in order to send them on missions that would otherwise make Gallia lose face in the war . While at times this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain members of the 422nd great distress . One such member , Gusurg , becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven , attached to the ideal of Darcsen independence proposed by their leader , Dahau . At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their own interests . Hounded by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort . This continues until the Nameless 's commanding officer , Ramsey Crowe , who had been kept under house arrest , is escorted to the capital city of Randgriz in order to present evidence exonerating the weary soldiers and expose the real traitor , the Gallian General that had accused Kurt of Treason .\n",
        "Partly due to these events , and partly due to the major losses in manpower Gallia suffers towards the end of the war with the Empire , the Nameless are offered a formal position as a squad in the Gallian Army rather than serve as an anonymous shadow force . This is short @-@ lived , however , as following Maximilian 's defeat , Dahau and Calamity Raven move to activate an ancient Valkyrian super weapon within the Empire , kept secret by their benefactor . Without the support of Maximilian or the chance to prove themselves in the war with Gallia , it is Dahau 's last trump card in creating a new Darcsen nation . As an armed Gallian force invading the Empire just following the two nations ' cease @-@ fire would certainly wreck their newfound peace , Kurt decides to once again make his squad the Nameless , asking Crowe to list himself and all under his command as killed @-@ in @-@ action . Now owing allegiance to none other than themselves , the 422nd confronts Dahau and destroys the Valkyrian weapon . Each member then goes their separate ways in order to begin their lives anew .\n",
        "= = Development = =\n",
        "Concept work for Valkyria Chronicles III began after development finished on Valkyria Chronicles II in early 2010 , with full development beginning shortly after this . The director of Valkyria Chronicles II , Takeshi Ozawa , returned to that role for Valkyria Chronicles III . Development work took approximately one year . After the release of Valkyria Chronicles II , the staff took a look at both the popular response for the game and what they wanted to do next for the series . Like its predecessor , Valkyria Chronicles III was developed for PlayStation Portable : this was due to the team wanting to refine the mechanics created for Valkyria Chronicles II , and they had not come up with the \" revolutionary \" idea that would warrant a new entry for the PlayStation 3 . Speaking in an interview , it was stated that the development team considered Valkyria Chronicles III to be the series ' first true sequel : while Valkyria Chronicles II had required a large amount of trial and error during development due to the platform move , the third game gave them a chance to improve upon the best parts of Valkyria Chronicles II due to being on the same platform . In addition to Sega staff from the previous games , development work was also handled by Media.Vision. The original scenario was written Kazuki Yamanobe , while the script was written by Hiroyuki Fujii , Koichi Majima , Kishiko Miyagi , Seiki Nagakawa and Takayuki Shouji . Its story was darker and more somber than that of its predecessor .\n",
        "The majority of material created for previous games , such as the BLiTZ system and the design of maps , was carried over . Alongside this , improvements were made to the game 's graphics and some elements were expanded , such as map layouts , mission structure , and the number of playable units per mission . A part of this upgrade involved creating unique polygon models for each character 's body . In order to achieve this , the cooperative elements incorporated into the second game were removed , as they took up a large portion of memory space needed for the improvements . They also adjusted the difficulty settings and ease of play so they could appeal to new players while retaining the essential components of the series ' gameplay . The newer systems were decided upon early in development . The character designs were done by Raita Honjou , who had worked on the previous Valkyria Chronicles games . When creating the Nameless Squad , Honjou was faced with the same problem he had had during the first game : the military uniforms essentially destroyed character individuality , despite him needing to create unique characters the player could identify while maintaining a sense of reality within the Valkyria Chronicles world . The main color of the Nameless was black . As with the previous Valkyria games , Valkyria Chronicles III used the CANVAS graphics engine . The anime opening was produced by Production I.G.\n",
        "= = = Music = = =\n",
        "The music was composed by Hitoshi Sakimoto , who had also worked on the previous Valkyria Chronicles games . When he originally heard about the project , he thought it would be a light tone similar to other Valkyria Chronicles games , but found the themes much darker than expected . An early theme he designed around his original vision of the project was rejected . He redid the main theme about seven times through the music production due to this need to reassess the game . The main theme was initially recorded using orchestra , then Sakimoto removed elements such as the guitar and bass , then adjusted the theme using a synthesizer before redoing segments such as the guitar piece on their own before incorporating them into the theme . The rejected main theme was used as a hopeful tune that played during the game 's ending . The battle themes were designed around the concept of a \" modern battle \" divorced from a fantasy scenario by using modern musical instruments , constructed to create a sense of atonality . While Sakimoto was most used to working with synthesized music , he felt that he needed to incorporate live instruments such as orchestra and guitar . The guitar was played by Mitsuhiro Ohta , who also arranged several of the later tracks . The game 's opening theme song , \" If You Wish for ... \" ( もしも君が願うのなら , Moshimo Kimi ga Negauno Nara ) , was sung by Japanese singer May 'n . Its theme was the reason soldiers fought , in particular their wish to protect what was precious to them rather than a sense of responsibility or duty . Its lyrics were written by Seiko Fujibayashi , who had worked on May 'n on previous singles .\n",
        "= = = Release = = =\n",
        "In September 2010 , a teaser website was revealed by Sega , hinting at a new Valkyria Chronicles game . In its September issue , Famitsu listed that Senjō no Valkyria 3 would be arriving on the PlayStation Portable . Its first public appearance was at the 2010 Tokyo Game Show ( TGS ) , where a demo was made available for journalists and attendees . During the publicity , story details were kept scant so as not to spoil too much for potential players , along with some of its content still being in flux at the time of its reveal . To promote the game and detail the story leading into the game 's events , an episodic Flash visual novel written by Fujii began release in January 2011 . The game was released January 27 , 2011 . During an interview , the development team said that the game had the capacity for downloadable content ( DLC ) , but that no plans were finalized . Multiple DLC maps , featuring additional missions and recruitable characters , were released between February and April 2011 . An expanded edition of the game , Valkyria Chronicles III Extra Edition , released on November 23 , 2011 . Packaged and sold at a lower price than the original , Extra Edition game with seven additional episodes : three new , three chosen by staff from the game 's DLC , and one made available as a pre @-@ order bonus . People who also owned the original game could transfer their save data between versions .\n",
        "Unlike its two predecessors , Valkyria Chronicles III was not released in the west . According to Sega , this was due to poor sales of Valkyria Chronicles II and the general unpopularity of the PSP in the west . An unofficial fan translation patch began development in February 2012 : players with a copy of Valkyria Chronicles III"
      ],
      "metadata": {
        "id": "Bdos3a31uqZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "662rVVgYuqXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sWnLPrJduqUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ibl4RId3uqQj"
      }
    }
  ]
}